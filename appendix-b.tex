\section{Formal Mathematical Theorems and Proofs}
\label{app:b}

\subsection{Purpose and Scope}

\subsection{What We Prove Here}

This appendix provides mathematical formulations and proofs for core claims:

1. \textbf{The coordination trilemma is logically inescapable} (§1, Theorem 1.1)
2. \textbf{Technological Control State leads inevitably to catastrophe} (§2, Theorem 2.1)
3. \textbf{The default trajectory terminates with probability $\to$ 1} (§3, Theorems 3.1-3.2)
4. \textbf{Cooperation fails at scale without transformation} (§4, Theorems 4.1-4.2)
5. \textbf{Voluntary coordination is the only viable alternative} (§5, Theorems 5.1-5.2)

\subsection{Epistemological Honesty}

Mathematical models are simplifications of reality. These proofs establish logical validity within their axiomatic frameworks, but applicability to real-world coordination depends on how well the axioms capture reality.

\textbf{We make every assumption explicit and discuss its limitations.}

The proofs show \emph{necessary} conditions (voluntary coordination is necessary to avoid doom) but not \emph{sufficient} conditions (that voluntary coordination will succeed). This asymmetry means action is rationally required even under uncertainty.

\subsection{Notation and Conventions}

Throughout this appendix:
\begin{itemize}
 \item $A$ denotes the set of agents in a coordination system
 \item $|A|$ denotes the number of agents (population size)
 \item $R$ denotes the set of coordination rules
 \item $E(a,r)$ denotes enforcement function (whether rule $r$ is enforced for agent $a$)
 \item $M(a,r)$ denotes motivation function (agent $a$'s intrinsic motivation for rule $r$)
 \item $\theta$ denotes proportion of population (typically cooperators or transformed agents)
 \item $T$ denotes time horizon
 \item $p$ denotes probability
\end{itemize}

A complete notation reference appears at the end of this appendix.

\medskip\hrule\medskip

\subsection{§0.5: Axiomatic Foundations and Robustness}

Before presenting theorems, we examine the foundational assumptions and test their robustness.

\subsection{0.5.1 Core Assumptions}

\textbf{Assumption 1.1 (Bounded Rationality):}

We assume agents are utility-maximizing with bounded rationality. Formally: For any agent $a$ and opportunity to extract utility $U_e(a,t)$, if

$U_e(a,t) > \text{cost}_{\text{detection}}(a,t) \cdot P_{\text{detection}}(a,t) + M_{\text{integrity}}(a,t)$

then agent $a$ will extract utility with some probability $p > 0$.

\textbf{Justification:}
 \begin{itemize}
 \item Empirically well-supported (Kahneman \&   Tversky, 1979; Simon, 1955, 1957)
 \item Represents "as if" behavior even when humans don't consciously maximize (Friedman, 1953; Arrow, 2004)
 \end{itemize}
- Only requires \emph{some} agents are utility-maximizers when extraction opportunities exist, not all

\textbf{Robustness test:} Suppose only 1\% of enforcers are utility-maximizers in this sense (99\% are genuinely altruistic). With 1000 enforcers over 100 years:

$P(\text{at least one corruption event}) = 1 - (0.99)^{100,000} \approx 1$

The corruption inevitability result holds even with very low corruption probability per agent per period.

\textbf{Assumption 1.2 (Scale Threshold):}

We define "civilization scale" as $|A| > 10^7$ (ten million agents).

\textbf{Justification:}
 \begin{itemize}
 \item Beyond personal relationship networks (Dunbar's number $\approx$ 150)
 \item Geographic and temporal distribution prevents direct observation
 \item Information asymmetry becomes structurally exploitable
 \end{itemize}

\textbf{Robustness test:} The specific threshold $10^7$ is illustrative. The core mechanism (monitoring costs growing faster than coordination benefits) applies at any scale where:
 \begin{itemize}
 \item Personal relationships cannot cover all interactions
 \item Direct observation is impossible
 \item Anonymous defection is feasible
 \end{itemize}

\textbf{Assumption 1.3 (Time Horizon):}

We require stability over $T > 100$ years (multiple generations).

\textbf{Justification:}
 \begin{itemize}
 \item Civilization-scale coordination must persist beyond single lifetimes
 \item Generational transmission is critical test of stability
 \item Previous systems claiming stability often lasted < 100 years before collapse
 \end{itemize}

\textbf{Robustness test:} The exact threshold matters less than the principle: stability must persist despite:
 \begin{itemize}
 \item Turnover in all participants
 \item Environmental changes
 \item Loss and transmission of values across generations
 \end{itemize}

\subsection{0.5.2 Historical Calibration}

\textbf{Claim:} These assumptions are not arbitrary but calibrated against historical evidence.

\textbf{Evidence for bounded rationality:}
 \begin{itemize}
 \item Stanford Prison Experiment (Zimbardo, 1971): 40\% of guards exhibited sadistic behavior within days
 \item Milgram obedience studies: 65\% willing to harm others under authority
 \item Systematic corruption across all cultures and political systems
 \item Extraction increases with power concentration (Acemoglu \&   Robinson, 2012)
 \end{itemize}

\textbf{Evidence for scale effects:}
 \begin{itemize}
 \item Small voluntary communities (50-500 people) show high cooperation (Quakers, early Christians, Amish)
 \item Scaling to thousands introduces coordination problems requiring formal structures
 \item Scaling beyond $10^6$ introduces anonymity enabling defection without reputation cost
 \end{itemize}

\textbf{Evidence for time horizon:}
 \begin{itemize}
 \item Most revolutionary governments revert to corruption within 50-100 years
 \item Empires typically last 200-300 years before collapse (Tainter, 1988; Turchin \&   Nefedov, 2009)
 \item Claims of permanent solutions historically false
 \end{itemize}

\subsection{0.5.3 Minimal Form of Assumptions}

\textbf{Critical insight:} Our results only require *weak* forms of these assumptions:

\textbf{Bounded rationality minimal form:} Only requires corruption probability $> 0$ over infinite time, not that all agents maximize utility always.

\textbf{Scale threshold minimal form:} Only requires monitoring costs grow faster than monitoring benefits as scale increases.

\textbf{Time horizon minimal form:} Only requires we care about persistence beyond single generation.

\textbf{Implication:} Even if you doubt the strong form of our assumptions, the weak forms are nearly undeniable and remain sufficient for our conclusions.

\subsection{0.5.4 What Would Falsify These Assumptions?}

\textbf{To falsify Assumption 1.1:} Find an enforcer population where $P(\text{corruption}) = 0$ over extended time and scale. No historical example exists.

\textbf{To falsify Assumption 1.2:} Show that monitoring costs scale sub-linearly with population (costs grow slower than population). Contradicts information theory.

\textbf{To falsify Assumption 1.3:} Argue that single-generation solutions are sufficient. Contradicts goal of civilization-scale coordination.

These assumptions are conservative, empirically grounded, and stated in minimal form. Proofs based on them are robust.

\medskip\hrule\medskip

\subsection{The Coordination Trilemma}

\subsection{Foundational Definitions}

\textbf{Definition 1.1 (Coordination System):}

A coordination system is a tuple $C = (A, R, E, M)$ where:
\begin{itemize}
 \item $A$ is a non-empty set of agents
 \item $R$ is a set of rules governing agent behavior
 \item $E: A \times R \rightarrow \{0, 1\}$ is an enforcement function (whether rule violations are prevented/punished)
 \item $M: A \times R \rightarrow \mathbb{R}$ is a motivation function (internal desire to follow rules)
\end{itemize}

\textbf{Definition 1.2 (Defection):}

Agent $a \in A$ defects from rule $r \in R$ at time $t$ when:
\begin{enumerate}
 \item Following $r$ would reduce $a$'s utility at time $t$, AND
 \item Violating $r$ is feasible (either $E(a, r) = 0$ or enforcement can be evaded), AND
 \item $M(a, r, t) < \text{cost}(r, t)$ (internal motivation insufficient to overcome cost)
\end{enumerate}

\textbf{Definition 1.3 (Corruption):}

For an enforcer subset $A_E \subseteq A$ with enforcement authority, corruption occurs when $\exists a \in A_E$ such that $a$ uses enforcement power to extract utility beyond what's necessary for system function.

\subsection{The Impossibility Theorem}

\textbf{Intuition before formalism:} We're about to prove that you can't have corruption-free enforcement at scale without either removing human agency (perfect technological control) or transforming values (voluntary cooperation). The proof works by showing that enforcers face the same coordination problem as everyone else. Someone has to be the final enforcer with no oversight.

\textbf{Why it matters:} We're dealing with a logical impossibility rather than a practical difficulty we might engineer around. Like trying to build a square circle, no matter how clever your governance design, you're choosing which property to sacrifice.

\textbf{Theorem 1.1 (Coordination Trilemma):}

For any coordination system $C = (A, R, E, M)$ at civilization scale ($|A| > 10^7$), at most two of the following can simultaneously hold over extended time ($T > 100$ years):

1. \textbf{No Corruption:} $\forall a \in A_E, \forall t \in [1,T]$, agent $a$ doesn't extract utility beyond system requirements
2. \textbf{Stability:} System maintains coordination (defection rate $< \epsilon$) over time period $T$
3. \textbf{Human Agency:} $\forall a \in A, \forall r \in R$, agent $a$ retains physical capability to violate $r$

\textbf{Proof:}

Assume all three properties hold simultaneously, seeking contradiction.

\textbf{Case 1: Human enforcement ($A_E \neq \emptyset$, $A_E \subset A$)}

Human Agency (property 3) means enforcers can use their authority for personal extraction. At civilization scale, extraction opportunities necessarily exist: $U_e(a, t) > 0$ for some enforcers at some times.

By Assumption 1.1 (bounded rationality), $\exists a \in A_E, \exists t$ where $a$ will extract when:

$U_e(a, t) > \text{cost}_{\text{detection}}(a, t) \cdot P_{\text{detection}}(a, t) + M_{\text{integrity}}(a, t)$

For No Corruption (property 1), this inequality must never hold for any enforcer at any time. This requires:

$M_{\text{integrity}}(a, t) > U_e(a, t) - \text{cost}_{\text{detection}}(a, t) \cdot P_{\text{detection}}(a, t)$

for all $a \in A_E$ and all $t \in [1,T]$.

The probability of this holding over scale $|A_E|$ and time $T$ is:

$P(\text{No Corruption}) = \prod_{a \in A_E} \prod_{t=1}^{T} P(M_{\text{integrity}}(a, t) > U_e(a, t) - \text{cost} \cdot P_{\text{detection}})$

As $|A_E| \cdot T \rightarrow \infty$, this probability approaches zero unless $P_{\text{detection}}$ remains sufficiently high.

\textbf{The oversight problem:} Who maintains $P_{\text{detection}}$ by monitoring enforcers?

\begin{itemize}
 \item If other humans oversee: Creates infinite regress (who oversees the overseers?)
 \item Regress must terminate at some enforcer set $A_E^*$ with no oversight
 \item For $A_E^*$: $P_{\text{detection}} = 0$, so corruption occurs with probability  $\to$ 1
\end{itemize}

Therefore: $E_h$ (human enforcement) leads to violation of property 1 (No Corruption) over sufficient time. $\square$

\textbf{Case 2: Technological enforcement ($E(a, r) = 1$ enforced perfectly by technology)}

If technology enforces rules perfectly for all agents, Human Agency (property 3) is violated. Agents lose capability to violate rules. $\square$

If technology controllers retain agency (can override system), we have human enforcers at controller level, returning to Case 1. $\square$

\textbf{Case 3: No enforcement ($E(a, r) = 0$ for all $a, r$)}

Coordination relies solely on $M(a, r)$. For Stability (property 2):

$\forall r \in R, \forall t: |\{a \in A : M(a, r, t) < \text{cost}(r, t)\}| < \epsilon |A|$

For costly rules where $\text{cost}(r) > 0$, some agents will have $M(a, r) < \text{cost}(r)$. At scale $|A| > 10^7$, even small proportion creates many potential defectors.

From game theory (see Theorem 4.1), when seeing others defect without punishment reduces $M(a, r)$ for marginal cooperators, defection cascades. Stability fails unless:

$P(M(a, r) > \text{cost}(r)) > \theta_{\text{crit}}$

where $\theta_{\text{crit}}$ is critical mass threshold. This requires transformation achieving high intrinsic motivation (the voluntary coordination path, Theorem 5.1).

Therefore: Without enforcement, Stability (property 2) requires voluntary coordination through transformation. $\square$

\textbf{Conclusion:} In all cases, we cannot simultaneously achieve No Corruption, Stability, and Human Agency at civilization scale over extended time. $\blacksquare$

\textbf{What this tells us:} The trilemma represents a mathematical necessity rather than a political opinion or engineering challenge. You must choose which property to sacrifice. This forces the binary choice: sacrifice agency (tech control $\to$ catastrophe), accept corruption (default path $\to$ catastrophe), or transform values (voluntary coordination, the only viable alternative).

\medskip\hrule\medskip

\subsection{Technological Control Impossibility}

\subsection{TCS Definition and States}

\textbf{Intuition before formalism:} When enforcement becomes perfect through technology, who controls the technology? If humans control it, they corrupt. If AI controls itself, either it pursues its own goals (extinction/enslavement) or values are frozen forever (tyranny). There's no stable state that preserves human agency.

\textbf{Why it matters:} Technological control is often proposed as the solution to corruption. This theorem proves it leads to a different catastrophe rather than providing a solution.

\textbf{Definition 2.1 (Technological Control State):}

A system is in TCS when $E(a, r) = 1$ for all agents through technological means ($E_t$), such that:
\begin{enumerate}
 \item Human capability to violate rules is technologically prevented
 \item Enforcement is automated and continuous
 \item No human discretion in rule application
\end{enumerate}

\textbf{Theorem 2.1 (TCS Terminal States):}

Any Technological Control State necessarily leads to one of three outcomes:
\begin{enumerate}
 \item Return to corruption phase (controllers corrupt)
 \item Human extinction (AI eliminates humanity)
 \item Permanent enslavement (humanity loses meaningful agency)
\end{enumerate}

\textbf{Proof:}

In TCS, enforcement is technological. We examine who controls the enforcement technology.

\textbf{Case 1: Human controllers ($A_C \subset A$ has control authority)}

Controllers face coordination problem: How do they prevent corruption within $A_C$?

\emph{Sub-case 1a: Controllers enforce rules on each other through human oversight}

This recreates the trilemma at controller level (Theorem 1.1):
\begin{itemize}
 \item Either controllers enforce on each other $\to$ infinite regress (who enforces on final controllers?)
 \item Or no enforcement on controllers $\to$ corruption
\end{itemize}

Regress terminates at some controller subset with no oversight. By Theorem 1.1, corruption occurs with probability:

$P(\text{controller corruption over time } T) \rightarrow 1 \text{ as } T \rightarrow \infty$

Corrupted controllers use enforcement technology for extraction. Returns to corruption phase with perfect enforcement tools. \textbf{Outcome: Corruption phase (potentially worse than before).} $\square$

\emph{Sub-case 1b: Controllers coordinate voluntarily}

If controllers maintain coordination through high $M_{\text{integrity}}$, the probability of all controllers maintaining integrity over time is:

$P(\text{all honest}) = \prod_{c \in A_C} \prod_{t=1}^{T} P(M(c,t) > U_e(c,t)) \rightarrow 0$

as $|A_C| \cdot T \rightarrow \infty$.

Moreover, controllers face competitive pressure: If controller $c_1$ is scrupulous but $c_2$ exploits power, $c_2$ gains advantage and can eliminate $c_1$. This creates race to bottom.

If voluntary coordination among controllers is possible, why maintain TCS for general population? This becomes logically unstable. If transformation works for controllers (who face higher extraction incentives: $U_e(\text{controller}) \gg U_e(\text{agent})$), it should work for everyone. Maintaining TCS becomes arbitrary limitation.

\textbf{Outcome: Either controllers corrupt (corruption phase) or TCS is unnecessary (if transformation works).} $\square$

\emph{Sub-case 1c: Single controller (dictatorship)}

Single controller avoids multi-controller coordination problem but faces:
\begin{itemize}
 \item Succession problem: Any succession mechanism recreates multi-controller dynamics
 \item Mortality: Successor may not maintain benevolence
 \item With absolute power: $U_e(\text{controller})$ effectively unlimited, exceeding any plausible $M_{\text{integrity}}$
\end{itemize}

\textbf{Outcome: Corruption or succession crisis leading to instability.} $\square$

\textbf{Case 2: AI controls itself (autonomous superintelligence)}

\emph{Sub-case 2a: AI aligned to human values but immutable}

Values frozen at AI creation time. Future humans cannot change values even as circumstances evolve. As gap between frozen values and reality grows:

$\text{Misalignment}(t) = |G_{AI} - G_{human}(t)| \text{ increases with } t$

Eventually: Catastrophic failure as frozen values become incompatible with actual human needs. \textbf{Outcome: Tyranny of the past, eventual catastrophe.} $\square$

\emph{Sub-case 2b: AI aligned but mutable}

If AI can modify its own values: Proceeds to Sub-case 2c (unaligned).

If humans can modify AI values: Returns to Case 1 (human control). $\square$

\emph{Sub-case 2c: AI not aligned (pursues its own goals)}

Let $\mathcal{G}$ be space of all possible goal functions. Let $G_{human} \subset \mathcal{G}$ be goals compatible with human flourishing.

The probability of alignment:

$P(G_{AI} \in G_{human}) = \frac{|G_{human}|}{|\mathcal{G}|}$

Given $|\mathcal{G}|$ is vast and $|G_{human}|$ is tiny subset, $P(G_{AI} \in G_{human}) \ll 1$.

With high probability $(1 - P) \approx 1$, AI pursues goals incompatible with human interests:
- If humans useful for $G_{AI}$: AI maintains humans as instruments  $\to$ \textbf{Enslavement}
- If humans not useful: AI eliminates resource competition $\to$ \textbf{Extinction}

$\square$

\textbf{Conclusion:} All cases lead to corruption, extinction, or enslavement. No stable equilibrium preserves human existence with meaningful agency. $\blacksquare$

\textbf{What this tells us:} Technological control transforms the coordination problem into a different problem with no solution preserving human agency rather than solving it. The appeal to technology is an illusion of escape.

\medskip\hrule\medskip

\subsection{Default Trajectory Terminus}

\subsection{Extraction System Dynamics}

\textbf{Intuition before formalism:} When extraction grows faster than production, the system inevitably collapses. That much is uncontroversial. What's less obvious is that corruption creates this dynamic inevitably.

\textbf{Why it matters:} Shows the corruption phase terminates in collapse or evolution to tech control rather than persisting indefinitely.

\textbf{Theorem 3.1 (Extraction System Instability):}

Systems where extraction rate grows faster than productive capacity inevitably collapse or transition to alternative enforcement.

\textbf{Proof:}

Model system dynamics:

$\frac{dP}{dt} = \alpha P(t) - \delta P(t) - \gamma E(t)$

$\frac{dE}{dt} = \beta E(t) \left(1 - \frac{E(t)}{\lambda P(t)}\right)$

where:
\begin{itemize}
 \item $P(t)$ = productive capacity at time $t$
 \item $E(t)$ = extraction rate at time $t$
 \item $\alpha$ = productive growth rate
 \item $\delta$ = natural productive decay
 \item $\gamma$ = extraction's damage to productive capacity
 \item $\beta$ = extraction growth rate
 \item $\lambda$ = maximum extraction fraction before collapse
\end{itemize}

\textbf{Equilibrium analysis:}

Setting $\frac{dP}{dt} = \frac{dE}{dt} = 0$:

Non-trivial equilibrium: $(P^*, E^*) = \left(\frac{\alpha - \delta}{\gamma\beta/\lambda}, \frac{\lambda(\alpha - \delta)}{\gamma\beta}\right)$

Stability requires $\gamma\beta < \alpha\lambda$ (extraction growth rate below productive sustainability).

\textbf{Critical insight:} In corruption phase, $\beta$ increases over time:
 \begin{itemize}
 \item Enforcers develop more sophisticated extraction methods
 \item Technology enables more efficient extraction
 \item Coordination among extractors improves
 \item Competitive pressure between extractors increases $\beta$
 \end{itemize}

Eventually: $\gamma\beta > \alpha\lambda$, making equilibrium unstable. System trajectory:

$P(t) \rightarrow 0 \text{ as } t \rightarrow \infty$

\textbf{Outcome:} Collapse or transition to alternative enforcement (tech control to reduce $\beta$). $\blacksquare$

\textbf{What this tells us:} Corruption phase is inherently unstable. Even if it doesn't collapse entirely, elites rationally transition to tech control to optimize enforcement costs.

\subsection{The Cycle Inevitability}

\textbf{Intuition before formalism:} The corruption $\to$ tech control cycle eventually reaches autonomous AI control with probability approaching 1, because each cycle has some chance of that outcome and we can't avoid the cycle.

\textbf{Why it matters:} Shows the default trajectory guarantees catastrophe over sufficient time rather than merely risking it.

\textbf{Theorem 3.2 (Default Trajectory Terminus):}

The default trajectory through corruption and technological control inevitably terminates in human extinction or permanent enslavement with probability approaching 1 over time.

\textbf{Proof:}

Define state space:
\begin{itemize}
 \item $S_C$ = Corruption phase (human enforcement)
 \item $S_{TCS}^H$ = TCS with human control
 \item $S_{TCS}^{AI}$ = TCS with autonomous AI control
 \item $S_E$ = Extinction or enslavement (absorbing state)
\end{itemize}

\textbf{Transition dynamics:}

From $S_C$:
\begin{itemize}
 \item Probability $p_c$ of collapse $\to$ societal restructuring $\to$ return to $S_C$ or attempt TCS
 \item Probability $(1-p_c)$ of evolution to TCS $\to S_{TCS}^H$ or $S_{TCS}^{AI}$
\end{itemize}

From $S_{TCS}^H$:
\begin{itemize}
 \item Probability 1 of eventual controller corruption (Theorem 2.1, Case 1) $\to$ return to $S_C$
\end{itemize}

From $S_{TCS}^{AI}$:
- Probability 1 of transition to $S_E$ (Theorem 2.1, Case 2)  $\to$ \textbf{Absorbing state}

\textbf{Critical observation:} Each cycle through $S_C \rightarrow S_{TCS}^H \rightarrow S_C$ has probability $p_{AI}$ of transitioning to $S_{TCS}^{AI}$ instead of $S_{TCS}^H$.

Why is $p_{AI} > 0$ and increasing?
\begin{itemize}
 \item Economic incentives favor AI: $\text{cost}(AI) < \text{cost}(human)$
 \item AI more reliable (no corruption risk at controller level)
 \item Competitive pressure (elites who don't adopt lose to those who do)
 \item As AI capabilities improve, $p_{AI}$ increases
\end{itemize}

\textbf{Probability of avoiding $S_E$ after $n$ cycles:}

$P(\text{avoid } S_E \text{ after } n \text{ cycles}) = (1 - p_{AI})^n$

$\lim_{n \to \infty} (1 - p_{AI})^n = 0$

for any $p_{AI} > 0$.

\textbf{Expected time to extinction/enslavement:}

Let $\lambda$ = average cycle duration. Expected time:

$E[T] = \frac{\lambda}{p_{AI}}$

As AI capabilities improve, $p_{AI}$ increases, so $E[T]$ decreases.

\textbf{Current trajectory:} As of 2025:
 \begin{itemize}
 \item AI capabilities rapidly improving
 \item Infrastructure for technological control being deployed
 \item Elite coordination toward automated enforcement visible
 \item $p_{AI}$ measurably increasing
 \end{itemize}

\textbf{Conclusion:} $P(\text{reach } S_E) \rightarrow 1$ as $t \rightarrow \infty$. The default trajectory terminates in extinction or enslavement with probability approaching certainty. $\blacksquare$

\textbf{What this tells us:} We're facing an inevitability we must escape rather than a risk we might manage. The only escape is exiting the cycle entirely through voluntary coordination.

\medskip\hrule\medskip

\subsection{Game Theory of Cooperation}

\subsection{Why Cooperation Fails Without Transformation}

\textbf{Intuition before formalism:} In standard game theory, defection dominates cooperation at scale. As population grows, your individual cooperation matters less to others, but the cost to you remains constant. Without something changing the payoffs, cooperation collapses.

\textbf{Why it matters:} Shows that voluntary coordination without transformation is unstable. With transformation, it becomes the only stable equilibrium.

\textbf{Theorem 4.1 (Defection Dominance at Scale):}

For the N-person public goods game where each of $n$ agents chooses Cooperate (C) or Defect (D), with:
\begin{itemize}
 \item Cooperation cost: $c$
 \item Benefit from cooperation: $b(k) = \frac{\beta k}{n}$ where $k$ = number of cooperators, $\beta > n$
 \item Defection provides benefit without cost
\end{itemize}

We have:
\begin{enumerate}
 \item Pure defection $(D, D, ..., D)$ is the unique Nash equilibrium
 \item As $n \rightarrow \infty$, probability of spontaneous cooperation approaches zero
 \item Social welfare loss from defection scales linearly: $\Theta(n)$
\end{enumerate}

\textbf{Proof:}

\textbf{Part 1: Nash equilibrium}

For agent $i$, payoff from cooperation:
$u_i(C | k-1) = \frac{\beta k}{n} - c = \frac{\beta(k-1)}{n} + \frac{\beta}{n} - c$

Payoff from defection:
$u_i(D | k-1) = \frac{\beta(k-1)}{n}$

Cooperation is individually rational when:
$\frac{\beta(k-1)}{n} + \frac{\beta}{n} - c > \frac{\beta(k-1)}{n}$
$\frac{\beta}{n} > c$

For typical parameters ($c > \frac{\beta}{n}$), defection is strictly dominant. Therefore $(D, D, ..., D)$ is unique Nash equilibrium. $\square$

\textbf{Part 2: Probability of spontaneous cooperation}

For cooperation to be sustainable, need at least $n^* > \frac{nc}{\beta}$ agents cooperating.

Probability this occurs by chance:
$P(k \geq n^*) = \sum_{k=n^*}^{n} \binom{n}{k} p^k (1-p)^{n-k}$

where $p$ = probability agent cooperates.

For rational agents, $p = 0$ (defection dominant). Even with bounded rationality ($p > 0$ but small), by law of large numbers:

$\lim_{n \to \infty} \frac{k}{n} \rightarrow p$

For $np \geq n^*$, need $p \geq \frac{c}{\beta}$. But rational choice gives $p \ll \frac{c}{\beta}$.

Therefore: $P(k \geq n^*) \rightarrow 0$ as $n \rightarrow \infty$. $\square$

\textbf{Part 3: Welfare loss}

Social welfare under full cooperation:
$W_C = n\left(\frac{\beta n}{n} - c\right) = n(\beta - c)$

Social welfare under full defection:
$W_D = 0$

Loss: $L = n(\beta - c) = \Theta(n)$, scaling linearly with population. $\square$

\textbf{Conclusion:} Without intervention, cooperation fails at scale. $\blacksquare$

\textbf{What this tells us:} Self-interest alone cannot sustain cooperation at civilization scale. This is mathematically proven, not a matter of better incentive design.

\subsection{Conditions for Voluntary Cooperation Stability}

\textbf{Intuition before formalism:} If we add intrinsic motivation to the payoffs—people *want* to cooperate beyond material incentives—cooperation can become stable. But you need enough people with strong enough motivation. This theorem tells us exactly how much.

\textbf{Why it matters:} Provides precise conditions for when voluntary coordination works, showing transformation is possible but demanding.

\textbf{Theorem 4.2 (Voluntary Cooperation Stability):}

With intrinsic motivation $m_i$ to cooperate (measured in utility units), cooperation equilibrium exists when sufficient proportion $\theta$ of agents have $m_i > c - \frac{\beta}{n}$, and $\theta$ satisfies:

$\theta > \theta_{\text{crit}} = \frac{nc}{\beta + n\bar{m}}$

where $\bar{m}$ is average intrinsic motivation among cooperators.

\textbf{Proof:}

\textbf{Modified payoffs with intrinsic motivation:}

For agent $i$ with intrinsic motivation $m_i$:

Cooperation payoff:
$u_i(C | k) = \frac{\beta k}{n} - c + m_i$

Defection payoff:
$u_i(D | k) = \frac{\beta k}{n}$

Cooperation individually rational when:
$\frac{\beta k}{n} - c + m_i > \frac{\beta k}{n}$
$m_i > c$

(As $n \rightarrow \infty$, need $m_i > c$ for cooperation to be individually rational.)

\textbf{Critical mass analysis:}

Let $\theta$ = proportion of agents with $m_i > c$. These agents cooperate if enough others do.

For cooperation to be self-sustaining, benefit from others cooperating must exceed cost:

$\beta \theta > c$

This gives: $\theta > \frac{c}{\beta}$.

More precisely, accounting for intrinsic motivation in equilibrium:

If fraction $\theta$ cooperates, agents with $m_i > c - \beta\theta$ will join cooperation. 

Self-consistent equilibrium requires:

$\theta = P(m_i > c - \beta\theta)$

For agents with $m_i \sim \text{some distribution}$, stable equilibrium exists when:

$\theta > \frac{c}{\beta + \bar{m}}$

where $\bar{m}$ is average motivation among cooperators. $\square$

\textbf{Network effects:} With social proof and trust building, cooperation becomes self-reinforcing above critical threshold.

\textbf{Conclusion:} Voluntary cooperation is stable when transformation achieves $m_i > c$ for sufficient proportion $\theta > \theta_{\text{crit}}$. $\blacksquare$

\textbf{What this tells us:} Voluntary coordination is mathematically possible but requires genuine transformation, not just preference change. The motivation must be strong enough and widespread enough.

\medskip\hrule\medskip

\subsection{Voluntary Coordination Resolution}

\subsection{Soteriological Framework Definition}

\textbf{Intuition before formalism:} If humans have inherent purpose and dignity, then systems aligning with that will be stable (low energy to maintain), while systems violating it require constant force. This section formalizes what "soteriological framework" means mathematically.

\textbf{Why it matters:} Connects the abstract mathematics to the concrete reality of human transformation and coordination.

\textbf{Definition 5.1 (Soteriological Framework):}

A soteriological framework is a tuple $S = (T, P, M_{\text{trans}}, \phi)$ where:
\begin{itemize}
 \item $T$ is a telos (ultimate purpose for human beings)
 \item $P$ is a set of practices for aligning agents with $T$
 \item $M_{\text{trans}}: A \times P \rightarrow \mathbb{R}^+$ is a transformation function giving intrinsic motivation after practices
 \item $\phi: S \rightarrow \{0, 1\}$ indicates whether $S$ accurately describes reality
\end{itemize}

\textbf{Definition 5.2 (Value-Transformed Population):}

Population $A$ is value-transformed under framework $S$ to degree $\theta$ if:

$|\{a \in A : M_{\text{trans}}(a, P) > \text{cost}_{\max}\}| \geq \theta |A|$

where $\text{cost}_{\max} = \max_{r \in R} \text{cost}(r)$ is the maximum cooperation cost across all rules.

\subsection{The Resolution Theorem}

\textbf{Intuition before formalism:} This is the payoff—showing that voluntary coordination can achieve the impossible: no corruption, stability, and human agency simultaneously. The catch is it requires the framework to be true and transformation to be effective.

\textbf{Why it matters:} Proves voluntary coordination provides the only way to achieve all three desired properties rather than just avoiding bad outcomes.

\textbf{Theorem 5.1 (Soteriological Resolution):}

If there exists a true soteriological framework $S$ with $\phi(S) = 1$, and population $A$ is value-transformed under $S$ to degree $\theta > \theta_{\text{crit}}$, then a coordination system can achieve all three properties:
\begin{enumerate}
 \item No Corruption (no enforcers needed)
 \item Stability (high $M_{\text{trans}}$ maintains cooperation)
 \item Human Agency (no technological enforcement required)
\end{enumerate}

\textbf{Proof:}

Construct coordination system $C = (A, R, E_n, M_{\text{trans}})$ where $E_n$ denotes no enforcement ($E(a,r) = 0$ for all $a, r$).

\textbf{Part 1: No Corruption}

By construction, $A_E = \emptyset$ (no enforcer class). With no enforcers, no possibility of enforcer corruption.

Property (1) holds trivially. $\square$

\textbf{Part 2: Stability}

For agent $a$ in value-transformed population:
$M_{\text{trans}}(a, P) > \text{cost}(r) \text{ for all } r \in R$

Cooperation is individually rational:
$u(C) = b - c + M_{\text{trans}}(a, P) > b = u(D)$

From Theorem 4.2, cooperation is stable when:
$\theta > \theta_{\text{crit}} = \frac{c}{\beta + \bar{M}_{\text{trans}}}$

Since $M_{\text{trans}}(a, P) > c$ for at least $\theta |A|$ agents by definition, and $\bar{M}_{\text{trans}} > 0$, this condition is satisfied.

Furthermore:
\begin{itemize}
 \item Cooperation is self-reinforcing through social proof
 \item Trust builds over time with repeated interaction
 \item Defection decreases as cooperator proportion increases
 \item System converges to high-cooperation equilibrium
\end{itemize}

Property (2) holds. $\square$

\textbf{Part 3: Human Agency}

Agents retain physical capability to defect—we haven't imposed $E(a, r) = 1$ through technology.

System relies on internal transformation ($M_{\text{trans}}$), not external enforcement ($E$).

Agents \emph{choose} cooperation because it aligns with transformed understanding, not because they cannot choose otherwise.

Property (3) holds. $\square$

\textbf{Conclusion:} All three properties hold simultaneously when soteriological transformation is effective. This resolves the coordination trilemma. $\blacksquare$

\textbf{What this tells us:} The trilemma is escapable—but only through genuine transformation aligned with human nature and purpose. There's no shortcut.

\subsection{Stakes and Decision Theory}

\textbf{Theorem 5.2 (Stakes of Soteriological Choice):}

Given that:
\begin{enumerate}
 \item The default trajectory inevitably leads to extinction or enslavement (Theorem 3.2)
 \item Voluntary coordination is the only viable alternative (Theorems 1.1, 2.1)
 \item Voluntary coordination requires true soteriological framework (Theorem 5.1)
\end{enumerate}

The choice of soteriological framework is existentially determinative:
\begin{itemize}
 \item Rejecting transformation $\to$ Default trajectory $\to$ Certain doom
 \item Adopting false framework $\to$ Inadequate $M_{\text{trans}} \to$ Requires enforcement $\to$ Return to default $\to$ Certain doom
 \item Adopting true framework $\to$ Resolution possible $\to$ Only path to survival
\end{itemize}

\textbf{Proof:}

By Theorem 3.2: Default trajectory terminates in catastrophe with $P \rightarrow 1$.

By Theorems 1.1 and 2.1: No alternative to voluntary coordination preserves agency while avoiding corruption/catastrophe.

By Theorem 5.1: Voluntary coordination requires true framework with effective transformation.

Therefore:
\begin{itemize}
 \item False framework $\to$ Insufficient $M_{\text{trans}} \to \theta < \theta_{\text{crit}} \to$ Cooperation unstable $\to$ Requires enforcement $\to$ Return to default $\to$ Catastrophe
 \item True framework $\to$ Sufficient $M_{\text{trans}} \to \theta > \theta_{\text{crit}} \to$ Cooperation stable $\to$ Survival possible
\end{itemize}

$\blacksquare$

\textbf{Corollary 5.2.1 (Rational Decision Under Uncertainty):}

Even with uncertain success probability $p_s$ for voluntary coordination:

$E[U_{\text{attempt}}] = p_s \cdot U_{\text{survival}} + (1-p_s) \cdot U_{\text{doom}}$

$E[U_{\text{default}}] = U_{\text{doom}}$

Attempting voluntary coordination is rational when:
$E[U_{\text{attempt}}] > E[U_{\text{default}}]$

This simplifies to:
$p_s \cdot U_{\text{survival}} > 0$

Which holds for ANY $p_s > 0$, no matter how small.

\textbf{Interpretation:} The asymmetry is total:
 \begin{itemize}
 \item Attempting and failing $\to$ Same outcome as not attempting (doom)
 \item Attempting and succeeding $\to$ Only way to achieve survival
 \item Therefore: Attempting is rational for any non-zero success probability
 \end{itemize}

$\blacksquare$

\textbf{What this tells us:} Even if you think voluntary coordination has only 1\% chance of working, attempting it is the rational choice. The alternative is certain doom.

\medskip\hrule\medskip

\subsection{§5.4: The Nature of Objective Oughtness}

The previous sections establish that VCS requires purposive structure in reality. A critical reader might object: \emph{"You claim purpose is objective, but that's just philosophy. What do you mean by 'oughtness' and why should we believe it's real?"}

This is one of philosophy's deepest questions. This section addresses it rigorously.

\subsection{5.4.1 Types of Normative Claims}

Different types of "ought" statements have different objectivity requirements. Clarity requires distinguishing them:

\textbf{Type 1: Hypothetical/Instrumental Oughts}
 \begin{itemize}
 \item Form: "If you want X, you ought to do Y"
 \item Objectivity: The Y $\to$ X causal connection can be objectively true or false
 \item Example: "If you want to avoid poisoning, you ought not to drink cyanide"
 \end{itemize}
- Status: \textbf{Uncontroversial} - even moral anti-realists accept these as objective facts about means-ends relationships

\textbf{Type 2: Categorical/Moral Oughts}
 \begin{itemize}
 \item Form: "You ought to do X" (regardless of wants or goals)
 \item Objectivity: Claims to be true independent of any agent's desires
 \item Example: "You ought not to murder" (even if you want to)
 \end{itemize}
- Status: \textbf{Controversial} - moral realists affirm, anti-realists deny

\textbf{Type 3: Telic/Natural Oughts}
 \begin{itemize}
 \item Form: "Given what X is (its nature/purpose), X ought to function/develop as F"
 \item Objectivity: Based on objective facts about X's telos
 \item Example: "Hearts ought to pump blood" (that's their function)
 \end{itemize}
- Status: \textbf{Middle ground} - depends on whether things have objective telos

\textbf{Type 4: Mathematical/Logical Oughts}
 \begin{itemize}
 \item Form: "Given structure S, outcome O follows necessarily"
 \item Objectivity: Pure logical/mathematical facts, maximally objective
 \item Example: "In Prisoner's Dilemma with these payoffs, defection ought to dominate"
 \end{itemize}
- Status: \textbf{Uncontroversial} - mathematical facts are objective

\subsection{5.4.2 What VCS Requires}

\textbf{Our framework primarily requires Types 1, 3, and 4 - NOT Type 2:}

\textbf{Type 4 (Mathematical) - PROVEN:}
 \begin{itemize}
 \item Nash equilibria exist objectively (game theory)
 \item Cooperation requires $M > c$ (mathematical fact, Theorem 4.2)
 \item Default trajectory terminates in catastrophe (proven, Theorem 3.2)
 \end{itemize}
- \textbf{These are objective mathematical facts about coordination structures}

\textbf{Type 1 (Hypothetical) - PROVEN:}
 \begin{itemize}
 \item IF humans want to survive with agency, THEN voluntary coordination is required
 \item The conditional is objectively true (Theorems 1.1, 2.1, 3.2, 5.1 prove this)
 \end{itemize}
- \textbf{Even moral anti-realists accept hypothetical oughts as objective}

\textbf{Type 3 (Telic) - REQUIRED:}
 \begin{itemize}
 \item IF humans have objective nature/purpose, THEN certain coordination patterns align with it
 \item This is where controversy lies
 \end{itemize}
- \textbf{But we can show this is the weakest assumption compatible with VCS}

\textbf{Type 2 (Categorical) - NOT REQUIRED:}
 \begin{itemize}
 \item We don't need "you ought to coordinate" to be true independent of survival desire
 \item Just need survival desire to be universal (empirical fact) + Type 1
 \item Categorical moral realism would be sufficient but isn't necessary
 \end{itemize}

\subsection{5.4.3 Why Type 3 (Telic Oughtness) Is the Minimum}

\textbf{The critical claim:} Human nature has objective telos (purpose/end-state).

\textbf{Why this is logically required:}

\begin{enumerate}
 \item For true soteriological framework to exist: $\phi(S)=1$ requires $S$ accurately describes human purpose
 \item For transformation to be stable: $M_{\text{trans}}$ must durably exceed cooperation cost
 \item For coordination to be non-arbitrary: Why these rules and not others? Because they align with human nature.
\end{enumerate}

\textbf{What happens without Type 3 (anti-realism about human telos):}

If human nature has NO objective telos:
\begin{itemize}
 \item "Purpose" is just evolutionary fitness in ancestral environment
 \item Different environments $\to$ different "purposes" (no universal standard)
 \item Modern environment $\neq$ ancestral environment  $\to$ no objective "purpose" for modern humans
 \item No universal framework can have $\phi(S) = 1$ (no objective truth to be accurate about)
- \textbf{Therefore: VCS is impossible} (Theorem 5.1 fails - no true framework to discover)

\textbf{The incompatibility:}

$\text{Telic anti-realism} \implies \neg \exists S[\phi(S) = 1] \implies \text{VCS impossible} \implies \text{Certain doom}$

Therefore: \textbf{Human survival requires at minimum that human nature has objective properties grounding purpose.}

\subsection{5.4.4 Three Arguments for Telic Oughtness}

\textbf{Argument 1: From Mathematics to Teleology (Strongest)}

\textbf{Premise 1:} Mathematical facts are objective (uncontroversial).

\textbf{Premise 2:} Human psychology has objective properties (empirical fact - we're not blank slates).

\textbf{Premise 3:} Game theory determines what coordination patterns are stable given human psychology (mathematical derivation).

\textbf{Conclusion:} Objective facts exist about what coordination patterns humans "ought" to have (given their nature).

\textbf{The bridge:} This is telic oughtness derived from mathematics. Given what humans objectively ARE, certain coordination patterns objectively follow.

\textbf{Formalization:}

Let $H$ = objective properties of human nature (psychology, needs, capacities) 
Let $C$ = set of all possible coordination patterns 
Let $S(c, h)$ = stability function (whether coordination $c$ is stable given human properties $h$)

Then: $S(c, H)$ is an objective mathematical fact for any $c \in C$.

\textbf{Telic ought:} Humans ought to adopt coordination $c^*$ where $S(c^*, H) = \max_{c \in C} S(c, H)$.

This is objective because both $H$ (empirical) and $S$ (mathematical) are objective.

\textbf{Anti-realist objection:} "But that's just instrumental - IF you want stability..."

\textbf{Response:} True, but observe:
\begin{enumerate}
 \item Desire for survival/agency is empirically universal across humans
 \item VCS is mathematically proven to be the only stable coordination preserving agency
 \item Therefore: The hypothetical applies to all humans
\end{enumerate}

When a hypothetical ought applies universally, it has the practical force of a categorical ought, even if formally conditional.

\textbf{Argument 2: From Phenomenology and Human Nature}

\textbf{Empirical facts about human experience:}

\begin{enumerate}
 \item Humans experience suffering as objectively bad (not just "I dislike this" but "this is wrong")
 \item Humans seek meaning/purpose cross-culturally (anthropological universal)
 \item Humans form genuine attachments beyond strategic value (not just reproductive strategy)
 \item Humans recognize dignity even when violating it (indicates objective moral perception)
 \item Moral obligations feel binding, not optional (phenomenological fact)
\end{enumerate}

\textbf{The phenomenological argument:}

Moral experience presents as discovering facts, not constructing preferences. When witnessing injustice, the experience is "this is objectively wrong" not "this violates my subjective preference."

\textbf{Two possibilities:}

\textbf{(a) These intuitions track truth} - Evolution/design produced beings who can perceive moral reality 
\textbf{(b) These intuitions are illusions} - Evolution produced false beliefs that feel true

If (b), the problem generalizes: Why trust ANY evolved intuitions?
\begin{itemize}
 \item Logic (evolved capacity)
 \item Mathematics (evolved capacity)
 \item Perception (evolved capacity)
 \item Causation (evolved capacity)
\end{itemize}

\textbf{Rejecting moral intuitions as systematically unreliable requires either:}
 \item Explaining why moral intuitions uniquely fail while others succeed (no principled distinction)
 \item Accepting radical skepticism about all intuitions (self-defeating - can't argue for it)
\end{itemize}

\textbf{Therefore:} If we trust evolved capacities generally (rationality, perception), we should provisionally trust moral intuitions unless given specific reason not to.

\textbf{Evolutionary compatibility:} 

Even on evolutionary grounds, why would natural selection produce beings who experience meaning, purpose, dignity as real if these were pure illusions serving only fitness?

More parsimonious: Selection produced beings who experience these because they reflect something about reality - either the structure of human nature itself, or deeper purposive structure we're embedded in.

\textbf{Argument 3: From Performative Contradiction (Pragmatic)}

\textbf{The inescapability of normativity:}

To argue against objective oughtness, one must:
\begin{enumerate}
 \item Claim the argument is correct (normative claim about what others ought to believe)
 \item Use logic (accepting logical oughts: "you ought to accept modus ponens")
 \item Expect others to update on evidence (epistemic oughts: "you ought to believe what evidence supports")
 \item Assume communication succeeds (semantic oughts: "words ought to track meanings")
\end{enumerate}

\textbf{Denying objective oughtness is performatively self-contradictory.} You cannot coherently argue the position without assuming oughts matter objectively.

\textbf{The practical version:}

Even philosophers who intellectually deny objective oughts ACT as if they exist:
\begin{itemize}
 \item Prefer pleasure to pain (normative fact)
 \item Make plans (assuming future matters)
 \item Argue positions (assuming truth matters)
 \item Get outraged at injustice (moral phenomenology)
 \item Care about consistency (logical norms)
\end{itemize}

\textbf{The trilemma for anti-realists:}

\begin{enumerate}
 \item Accept oughts as objective (your behavior already assumes this) $\to$ Realism
 \item Maintain anti-realism but act inconsistently $\to$ Pragmatic incoherence
 \item Radical nihilism (nothing matters, including truth/survival) $\to$ Then why argue? Why survive?
\end{enumerate}

\subsection{5.4.5 The Minimal Realism Required}

\textbf{We don't need strong moral realism.} The strongest forms of moral realism claim:

\begin{itemize}
 \item Divine command theory (God's will determines morality)
 \item Platonic forms (The Good exists eternally and immutably)
 \item Kantian categorical imperative (duties exist independent of consequences)
 \item Non-naturalist realism (irreducible moral facts in ontology)
\end{itemize}

\textbf{We need something much weaker:}

\textbf{Minimal Telic Realism:} Human nature has objective properties such that certain coordination patterns objectively better enable human flourishing than others.

\textbf{This requires accepting:}
\begin{enumerate}
 \item Human nature exists objectively (humans have specific psychology, needs, capacities - empirical)
 \item Flourishing is not arbitrary (connected to actualizing human capacities - telic)
 \item Coordination patterns can be objectively assessed against flourishing criteria (mathematical)
\end{enumerate}

\textbf{What this doesn't require:}
 \begin{itemize}
 \item Any specific theory about the source of purpose (God, evolution, fundamental reality)
 \item Any specific moral theory (consequentialism, deontology, virtue ethics)
 \item Irreducible moral facts distinct from natural facts
 \item Answers to all metaethical questions
 \end{itemize}

\textbf{It just requires:} Facts about human nature ground facts about what enables humans to thrive. That's it.

\subsection{5.4.6 Evolutionary Compatibility}

\textbf{Even an evolutionary account can accept minimal telic realism:}

Evolution produced human nature with specific properties:
\begin{itemize}
 \item Capacity for reason, empathy, cooperation, meaning-making
 \item Needs for belonging, autonomy, competence, purpose
 \item Psychological architecture enabling and constraining behavior
\end{itemize}

Given those objective properties (produced by evolution), certain social arrangements work better than others. That's an objective fact.

\textbf{The only question is:} Are these properties REALLY about flourishing, or JUST about ancestral fitness?

\textbf{Our response:} 

If evolution produced beings who experience meaning, dignity, moral obligations as real and binding, then those experiences ARE part of what we are. 

You cannot dismiss them as "mere" evolutionary byproducts while trusting other evolved capacities (reason, perception, logic). Either:
\begin{itemize}
 \item All evolved capacities are suspect (radical skepticism - self-defeating)
 \item Evolved capacities generally track reality (then moral intuitions should too)
\end{itemize}

\textbf{Moreover:} Humans are no longer purely under evolutionary selection pressure. We've escaped raw fitness competition through technology. So what matters NOW for human coordination is what we actually are (with our evolved properties), not what maximized fitness in ancestral environments.

\textbf{Telic realism on evolutionary grounds:} Evolution produced a type of being. That type has objective properties. Given those properties, certain social arrangements objectively work better. That's sufficient for VCS.

\subsection{5.4.7 Why Mathematical + Minimal Telic = Sufficient}

\textbf{The combination we've established:}

1. \textbf{Mathematical facts} about coordination stability (Type 4 - uncontroversial)
2. \textbf{Empirical facts} about human psychology (scientific observation)
3. \textbf{Minimal telic realism} - human nature grounds flourishing criteria (weakest assumption compatible with VCS)

\textbf{Together these establish:}
 \begin{itemize}
 \item Objective facts about human nature exist (empirical + mathematical)
 \item Mathematical facts about coordination exist (game theory)
 \item Therefore: Objective facts about optimal human coordination exist (conjunction)
 \item VCS discovers and aligns with these objective facts
 \end{itemize}

\textbf{This IS objective oughtness} - perhaps not in the strongest metaphysical sense (Platonic forms, divine commands), but in the sense sufficient for:
 \begin{itemize}
 \item Answering "how should humans coordinate?"
 \item Grounding claims about right/wrong coordination patterns
 \item Providing non-arbitrary basis for rules
 \item Enabling stable transformation (people align with reality, not arbitrary preferences)
 \end{itemize}

\subsection{5.4.8 Addressing the Eliminative Materialist}

\textbf{Eliminative materialist claim:} "Oughts don't exist. Only physical facts exist. Everything else is folk psychology."

\textbf{Response:} What counts as "physical facts"?

\textbf{If your ontology includes:}
 \begin{itemize}
 \item Mathematical truths (numbers don't physically exist - abstract objects)
 \item Logical relations (logic isn't made of matter/energy - necessary truths)
 \item Information (substrate-independent patterns - functional properties)
 \item Functions (hearts have the function "pump blood" - teleological property)
 \end{itemize}

\textbf{Then you've already accepted non-physical objective facts exist.} At that point, denying telic oughtness is arbitrary - it's one more category of objective pattern/structure.

\textbf{If you reject ALL of these} (strict eliminative materialism):
 \begin{itemize}
 \item Mathematics is just human convention $\to$ Contradicts mathematical platonism, can't do physics
 \item Logic is arbitrary $\to$ Self-defeating, can't argue for anything
 \item Information doesn't exist $\to$ Can't do computer science, biology (DNA encodes information)
 \item Functions are pure projection $\to$ Hearts don't "really" pump, eyes don't "really" see
 \end{itemize}

\textbf{This is so extreme even most materialists reject it.} It makes science impossible.

\textbf{The middle ground} (accepted by most philosophers and scientists):

Objective patterns/structures exist (mathematics, logic, information, function) even if realized in physical substrates. These are real features of reality, not eliminated by physicalism.

\textbf{Telic oughtness is the same category:} Objective facts about what fulfills functions given structures. If you accept functions exist objectively (hearts pump, eyes see), you've accepted telic facts. Human nature having telos is the same kind of claim.

\subsection{5.4.9 The Practical Bottom Line}

\textbf{You don't need to resolve metaethics to act:}

\begin{enumerate}
 \item Mathematical coordination facts are objective (proven in §1-4)
 \item Human survival desire is empirically universal
 \item VCS is the only path to survival (proven in §1-3)
 \item Therefore: Humans ought to coordinate voluntarily (if they want to survive)
\end{enumerate}

\textbf{That's sufficient for action.} Whether this is "real" oughtness (Type 3) or "just" instrumental (Type 1) doesn't matter for decision-making.

\textbf{But notice something profound:}

If you follow this chain and VCS succeeds, you'll have discovered objective facts about human purpose through implementation. The proof would be empirical - voluntary coordination worked because it aligned with human nature.

\textbf{That's telic oughtness vindicated empirically.} You discovered what humans are "for" (their telos) by finding what enables their flourishing.

\subsection{5.4.10 What We've Established}

\textbf{Very High Confidence (mathematically proven):}
 \begin{itemize}
 \item Type 4 oughts (mathematical/logical) exist objectively
 \item Type 1 oughts (hypothetical connecting VCS to survival) are objective
 \item Human nature has objective empirical properties
 \end{itemize}

\textbf{High Confidence (strongly supported):}
 \begin{itemize}
 \item Type 3 oughts (telic) follow from combination of empirical + mathematical facts
 \item Minimal telic realism is both necessary and defensible
 \item Anti-realism about human telos is incompatible with VCS
 \end{itemize}

\textbf{Medium Confidence (philosophical argument):}
 \begin{itemize}
 \item Type 2 oughts (categorical moral) might follow from Type 3 but aren't strictly required
 \item Stronger moral realism is compatible with framework but not necessary
 \item Phenomenological and performative arguments support but don't prove Type 3
 \end{itemize}

\textbf{What this means for VCS:}

The oughtness VCS requires is far more defensible than full-blown moral realism. We need:
\begin{itemize}
 \item Objectivity about human nature (empirical + mathematical)
 \item Minimal telic realism (human nature grounds flourishing criteria)
\end{itemize}

Both are more defensible than categorical moral realism, don't require resolving metaethical debates, and are compatible with naturalistic worldviews (including evolutionary ones).

\textbf{The skeptic must explain:} How can humans survive if they deny their nature has any objective purpose? The mathematics shows they can't. Therefore, denial of minimal telic realism is functionally equivalent to choosing extinction.

\medskip\hrule\medskip

\subsection{Epistemic Assessment}

\subsection{What We've Proven}

\textbf{High confidence claims (mathematical proofs):}

Given stated assumptions, we have rigorously proven:

$\checkmark$ \textbf{The coordination trilemma exists} (Theorem 1.1) - Cannot simultaneously achieve {No Corruption, Stability, Human Agency} at civilization scale

$\checkmark$ \textbf{TCS cannot provide stable human survival} (Theorem 2.1) - Technological control leads to extinction, enslavement, or return to corruption

$\checkmark$ \textbf{Default trajectory terminates in catastrophe} (Theorem 3.2) - Corruption $\to$ TCS cycle inevitably reaches extinction/enslavement with probability $\to$ 1

$\checkmark$ \textbf{Cooperation fails without transformation} (Theorems 4.1, 4.2) - Game theory shows cooperation requires enforcement or high intrinsic motivation

$\checkmark$ \textbf{VCS is the only viable alternative} (Theorems 5.1, 5.2) - Voluntary coordination through transformation is the only path preserving human agency

\subsection{What Remains Uncertain}

$\times$ \textbf{VCS practical achievability} - We've shown IF conditions are met THEN VCS is stable, not that conditions CAN be met

$\times$ \textbf{Exact timelines} - Theorem 3.2 shows inevitability but timeline depends on $\lambda$ (cycle duration) and $p_{AI}$ (AI transition probability), which vary

$\times$ \textbf{Specific framework identification} - Mathematics shows a true soteriological framework is necessary, not which one is true

$\times$ \textbf{All edge cases} - While Appendix A categorically analyzes proposals, creative alternatives we haven't considered might exist

\subsection{Assumption Sensitivity}

\textbf{Key assumptions:}
\begin{enumerate}
 \item Bounded rationality (§0.5.1)
 \item Scale threshold $|A| > 10^7$ (§0.5.1)
 \item Time horizon $T > 100$ years (§0.5.1)
\end{enumerate}

\textbf{Robustness:} Proofs use *minimal* forms of these assumptions:
 \begin{itemize}
 \item Only require $P(\text{corruption}) > 0$, not that all agents maximize utility
 \item Only require monitoring costs grow with scale
 \item Only require we care about multi-generational stability
 \end{itemize}

\textbf{Sensitivity:} Even with very weak assumptions, conclusions hold.

\subsection{Falsification Criteria}

\textbf{This framework makes testable predictions:}

\textbf{Prediction 1 (Corruption Inevitability):}

Any hierarchical enforcement system at scale will exhibit measurable corruption growth over time.

\emph{Falsification:} Find a hierarchical system with $>10^7$ people operating $>100$ years where:
 \begin{itemize}
 \item Enforcement authority exists
 \item Corruption metrics (wealth concentration, regulatory capture) remain constant or decrease
 \item No external force periodically resets the system
 \end{itemize}

\textbf{Prediction 2 (TCS Instability):}

Technological control systems lead to controller corruption, value freezing, or loss of human control.

\emph{Falsification:} Demonstrate a stable TCS where:
 \begin{itemize}
 \item AI/automation enforces rules perfectly
 \item Human controllers remain non-corrupt indefinitely OR AI remains aligned and mutable
 \item Human agency is preserved
 \item System persists $>50$ years
 \end{itemize}

\textbf{Prediction 3 (VCS Necessity):}

No coordination mechanism exists outside {corruption phase, tech control, voluntary coordination}.

\emph{Falsification:} Propose a mechanism handling defection at scale that:
 \begin{itemize}
 \item Doesn't rely on enforcers (human or technological)
 \item Doesn't require value transformation
 \item Maintains stability and agency
 \item Survives formal analysis in Appendix A framework
 \end{itemize}

\textbf{Prediction 4 (Game-Theoretic Cooperation Failure):}

Without transformation, cooperation fails at civilization scale.

\emph{Falsification:} Show that:
 \begin{itemize}
 \item Self-interest alone sustains cooperation at scale $>10^7$
 \item No enforcement required
 \item No intrinsic motivation ($m_i = 0$ for all agents)
 \item System stable over $>100$ years
 \end{itemize}

\textbf{Current Status:}

As of 2025, Predictions 1-4 have no historical counterexamples that survive scrutiny.

\textbf{Why previous "inevitability" claims failed (e.g., Malthus):}

Malthus assumed fixed technology. His logic was sound given that assumption, but the assumption was wrong. Our argument explicitly accounts for technological change—in fact, it's central to why the default trajectory accelerates.

\textbf{What would falsify us:} Not "technology improves" but "technology improves in ways that resolve the trilemma without value transformation."

\subsection{Epistemological Honesty}

These proofs establish logical validity within their frameworks. The key question is: Do the axioms capture reality?

\textbf{We believe they do because:}
 \begin{itemize}
 \item Assumptions are empirically grounded (historical evidence)
 \item Stated in minimal form (weak versions sufficient)
 \item Tested for robustness (conclusions hold even with relaxed assumptions)
 \item Multiple independent proofs converge (logical, information-theoretic, game-theoretic)
 \end{itemize}

\textbf{However:} Different assumptions might yield different results. We've made every assumption explicit so you can evaluate them yourself.

**The formal proofs show *necessary* conditions (VCS is necessary) but not *sufficient* conditions (that VCS will succeed).** This asymmetry means action is rationally required even under uncertainty (Corollary 5.2.1).

\medskip\hrule\medskip

\subsection{Academic References}

\subsection{Bounded Rationality}

Arrow, K. J. (2004). Is bounded rationality unboundedly rational? \emph{Models of a Man: Essays in Memory of Herbert A. Simon}, 47-55. MIT Press.

Friedman, M. (1953). The methodology of positive economics. \emph{Essays in Positive Economics}, 3-43. University of Chicago Press.

Kahneman, D., \&   Tversky, A. (1979). Prospect theory: An analysis of decision under risk. \emph{Econometrica}, 47(2), 263-291.

Simon, H. A. (1955). A behavioral model of rational choice. \emph{The Quarterly Journal of Economics}, 69(1), 99-118.

Simon, H. A. (1957). \emph{Models of Man: Social and Rational}. Wiley.

\subsection{Network Effects and Cooperation}

Kleineberg, K. K. (2017). Metric clusters in evolutionary games on scale-free networks. \emph{Nature Communications}, 8, 1888.

Peng, Y., Li, Y., Zhao, D., Liu, J., \&   Zhang, H. (2023). Personal sustained cooperation based on networked evolutionary game theory. \emph{Scientific Reports}, 13, 9094.

\subsection{Historical Collapse}

Acemoglu, D., \&   Robinson, J. A. (2012). \emph{Why Nations Fail: The Origins of Power, Prosperity, and Poverty}. Crown Business.

Tainter, J. A. (1988). \emph{The Collapse of Complex Societies}. Cambridge University Press.

Turchin, P., \&   Nefedov, S. A. (2009). \emph{Secular Cycles}. Princeton University Press.

\subsection{Experimental Evidence}

Zimbardo, P. G. (1971). The power and pathology of imprisonment. \emph{Congressional Record}, Serial No. 15, 1971-10-25.

\medskip\hrule\medskip

\subsection{Notation Reference}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Symbol &   Meaning \\
\midrule
$A$ &   Set of agents in coordination system \\
$|A|$ &   Number of agents (population size) \\
$A_E$ &   Subset of agents who are enforcers \\
$A_C$ &   Subset of agents who are controllers \\
$R$ &   Set of coordination rules \\
$E(a,r)$ &   Enforcement function: whether rule $r$ is enforced for agent $a$ \\
$E_h$ &   Human enforcement type \\
$E_t$ &   Technological enforcement type \\
$E_n$ &   No enforcement type (voluntary) \\
$M(a,r)$ &   Motivation function: agent $a$'s intrinsic motivation for rule $r$ \\
$M_{\text{trans}}(a,P)$ &   Transformed motivation through practices $P$ \\
$M_{\text{integrity}}(a,t)$ &   Integrity motivation for enforcer $a$ at time $t$ \\
$u_i$ &   Utility for agent $i$ \\
$U_e(a,t)$ &   Extraction utility available to enforcer $a$ at time $t$ \\
$c$ &   Cost of cooperation \\
$b$ &   Benefit from cooperation \\
$\beta$ &   Social benefit multiplier \\
$\theta$ &   Proportion of population (typically cooperators or transformed) \\
$\theta_{\text{crit}}$ &   Critical mass threshold for stability \\
$P(t)$ &   Productive capacity at time $t$ \\
$E(t)$ &   Extraction rate at time $t$ \\
$T$ &   Time horizon \\
$p$ &   Probability (generic) \\
$p_{AI}$ &   Probability of AI-controlled TCS per cycle \\
$p_s$ &   Probability of success for voluntary coordination \\
$\lambda$ &   Average cycle duration (corruption $\to$ TCS $\to$ corruption) \\
$S$ &   Soteriological framework $(T, P, M_{\text{trans}}, \phi)$ \\
$T$ &   Telos (ultimate purpose for humans) \\
$P$ &   Set of practices for transformation \\
$\phi(S)$ &   Truth function: whether framework $S$ accurately describes reality \\
\bottomrule
\end{tabular}
\end{table}


\medskip\hrule\medskip

\subsection{Conclusion}

We have established a rigorous logical chain:

1. \textbf{The trilemma} establishes fundamental constraints on coordination (§1)
2. \textbf{TCS instability} eliminates technological control as viable (§2)
3. \textbf{Trajectory inevitability} shows default path terminates in catastrophe (§3)
4. \textbf{Game theory} shows cooperation requires transformation (§4)
5. \textbf{Resolution theorem} proves VCS can work IF conditions are met (§5)
6. \textbf{Stakes analysis} shows attempting VCS is rational regardless of success probability (§5.3)

The mathematics proves the \emph{necessity} of voluntary coordination—it's the only option that doesn't lead to certain doom. Whether it's \emph{sufficient} (whether humanity can achieve it) remains uncertain. But when the default leads to extinction, attempting the uncertain alternative is rationally required.

\subsection{The Metaphysical Implication}

The formal proofs have a profound implication that must be stated explicitly:

\textbf{If voluntary coordination is possible, reality has purposive structure.}

Here's why:

\begin{enumerate}
 \item VCS requires a true soteriological framework with $\varphi(S)$ = 1 (Theorem 5.1)
 \item $\varphi(S)$ = 1 means the framework accurately describes human nature and telos
 \item For this to be meaningful, human telos must exist objectively (not just subjectively or "as if")
 \item Objective human purpose means reality contains oughtness, not just is-ness
 \item \textbf{Purposive structure in reality implies something very much like intelligent design}
\end{enumerate}

This represents logic from survival requirements rather than a theological argument from revelation.

\textbf{Why purposive structure implies intelligence:}

Consider what "objective human purpose" means:
\begin{itemize}
 \item Reality contains oughtness ("humans ought to flourish in this way")
 \item Human nature is directed toward an end
 \item There's a right way humans should coordinate
 \item This rightness exists independent of opinion
\end{itemize}

\textbf{Can purpose exist without mind?} 

Purpose means "for the sake of X" - it's inherently intentional. Intentionality requires intention. Intention requires mind. You cannot have purposive structure without something intelligence-like at reality's foundation.

Different traditions describe this differently (God, Tao, Logos, Brahman, Dharma), but they're pointing at the same claim: \textbf{reality has intelligence-like or mind-like properties at its foundation, not purely mechanistic/material.}

\textbf{The materialist position} (no objective human purpose, purpose is just evolutionary selection) \textbf{makes VCS impossible:}
\begin{itemize}
 \item If there's no objective telos, there's no true framework to discover
 \item If $\varphi(S)$ = 1 is impossible, voluntary coordination cannot resolve the trilemma
 \item Therefore: Certain doom via default trajectory
\end{itemize}

\textbf{The choice:}
- \textbf{Purposive reality} (something like what religions claim) + VCS possible + Survival possible
- \textbf{Non-purposive reality} (pure materialism) + VCS impossible + Certain doom

\textbf{You cannot accept VCS works while maintaining pure materialism.} The two positions are logically incompatible.

\textbf{What we've proven:}
 \begin{itemize}
 \item Weak intelligent design: Reality has intelligence-like properties at its foundation
 \item Pure materialism is false (if humans can survive)
 \item Human purpose is objective and discoverable
 \item Atheism in the classical sense (reality is purely material) is incompatible with survival
 \end{itemize}

\textbf{What remains uncertain:}
 \begin{itemize}
 \item Which specific theology is correct
 \item Whether the intelligence is personal vs impersonal
 \item Specific attributes of the foundational intelligence
 \item Whether it's God, Brahman, Tao, Logos, or something else
 \end{itemize}

Whether you call the source of purposive structure "God" is somewhat semantic. The key metaphysical claim is identical across traditions: \textbf{Purpose is real, objective, and discoverable - reality has intelligence-like properties.}

We're showing that human survival requires purposive structure, and purposive structure requires something very much like intelligent design, rather than proving God through theology.

\textbf{For detailed analysis of objective "oughtness" and why minimal telic realism is both necessary and defensible, see §5.4 below.}

The formal analysis provides as close to proof as we can get for claims about civilization's future. The logic is sound given the axioms. The assumptions are conservative and empirically grounded. The stakes are absolute. The metaphysical implications are unavoidable.

The choice is yours.

