\section{Synthetic Media and Epistemic Collapse}
\label{app:d}

\subsection{Executive Summary}

Within 3-6 years, synthetic media will make routine verification of content authenticity exponentially harder, closing the window for voluntary coordination based on verifiable truth. This appendix provides technical evidence for this claim, analyzes the trajectory, examines proposed countermeasures, and assesses timeline uncertainty honestly. The stakes are clear: voluntary coordination requires shared reality, shared reality requires verifiable truth, and verifiable truth requires the ability to distinguish real from synthetic content.

\bigskip

As of October 2025, generation capabilities have advanced dramatically: video generation now produces 20 seconds of 1080p with synchronized audio (OpenAI Sora 2), the open-source gap with commercial models decreased from 4.52\% to 0.69\% in six months, and state control is becoming impossible as consumer hardware can generate deepfakes.

Detection performance has deteriorated catastrophically. Human detection overall achieves only 55.54\% accuracy (barely above chance), while human detection for high-quality short clips has fallen to approximately 25\% (essentially failed). AI detection on real-world deepfakes shows 45-50\% performance drop versus academic benchmarks, with best real-world AI detection achieving only approximately 82\% AUC (versus 95\%+ on academic datasets). The gap is widening: each generation improvement requires detector retraining, but detectors cannot train on techniques that don't exist yet.

\textbf{Timeline with confidence levels.}

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
Claim &   Confidence &   Timeline \\
\midrule
Short-form video (<20s) crossed public threshold &   Very High (>90\%) &   Already occurred \\
Open-source will close gap with commercial &   Very High (>90\%) &   Ongoing \\
AI detection degrades on real-world content &   Very High (>90\%) &   Demonstrated \\
Economic incentives favor generation &   Very High (>90\%) &   Structural \\
Expert detection fails for most content &   High (>80\%) &   3-6 years \\
Verification becomes exponentially harder &   High (>80\%) &   3-6 years \\
Feature-length generation viable &   Low (<50\%) &   2028-2035 range \\
\bottomrule
\end{tabular}
\end{table}


\bigskip

Countermeasures will likely fail for structural reasons. Cryptographic content authentication requires universal hardware replacement costing trillions of dollars and taking decades, faces a bootstrapping problem where transition cannot be coordinated when information cannot be trusted, and remains vulnerable to state-level actors who can compromise hardware and mandate backdoors while raising questions about who controls verification infrastructure. AI detection improvements face structural disadvantage because generators see detectors and iterate faster, confront a 1000:1 funding disparity favoring generation, and approach a mathematical limit where as generators approach perfection, detection becomes theoretically impossible. Cultural adaptation is too slow (generations versus years), and extreme skepticism prevents coordination as much as credulity does, while previous media revolutions took decades that we don't have.

\bigskip

After the threshold, coordination becomes impossible: you cannot verify traditions against source texts because texts can be fabricated, cannot see institutional betrayals clearly because evidence is dismissed as "deepfakes," cannot coordinate around observable truth because truth becomes unknowable, and cannot build trust networks because no foundation for verification exists. Voluntary coordination requires shared reality, shared reality requires verifiable truth, and that window is closing.

This timeline would be falsified if detection accuracy improves faster than generation quality for 3+ consecutive years, if cryptographic signing achieves greater than 80\% market adoption by 2030, if verification cost decreases relative to generation cost, or if a fundamental new detection approach emerges that generators cannot evade. Current status: all metrics are moving in the predicted direction with no indication of reversal.

The asymmetry of outcomes determines rational action. If we are wrong pessimistically (window is 10 years, not 3), there is no harm from acting early. If we are wrong optimistically (window is 3 years, not 10), catastrophic harm results from delay. The rational choice is to act as if the aggressive timeline is correct. You can examine beliefs while truth is verifiable, or wait until it's impossible. This appendix proves the window is closing.


\subsection{Current State (October 2025)}

\textbf{Generation capabilities.}

\textbf{Video Generation}

The field has advanced dramatically in 2025:

\textbf{OpenAI Sora 2~\cite{openai2025sora2,openai2025sora2systemcard} (September 30, 2025)} generates up to 20 seconds of 1080p video from text prompts with synchronized audio generation including dialogue, sound effects, and ambient audio. Physics simulation has significantly improved compared to Sora 1: basketball rebounds now follow actual physics rather than "teleporting" to the hoop, with improved momentum, collisions, buoyancy, and rigidity modeling that better adheres to real-world dynamics. Character and object tracking remains consistent across frames. The main remaining artifacts are occasional physics violations and consistency issues across cuts.

\textbf{Open-source alternatives} have closed the gap rapidly. Open-Sora v1.2 decreased its performance gap with commercial Sora from 4.52\% (October 2024) to 0.69\% (March 2025). This rapid convergence means state control of generation technology is becoming impossible---anyone with consumer hardware (RTX 4090) can generate high-quality deepfakes locally.

\textbf{Feature-length generation claims:}
Some industry figures have claimed feature-length movie generation by 2026-2027. Current proven capability is 6-20 second clips. Feature-length represents 300-900x scaling with no demonstrated intermediate milestones. 

\textbf{Skeptical assessment:} More realistic estimate is 2028-2035 range, with high uncertainty. Claims made via social media without technical roadmap. Critical gap exists between demonstrated capability (20 seconds) and claimed trajectory (90+ minutes).

\textbf{Audio Generation}

Voice cloning has reached practical indistinguishability. ElevenLabs and Vall-E (Microsoft) require only 3 seconds of reference audio to clone a voice, with real-time voice conversion achieving less than 100ms latency. Entirely synthetic voices are now indistinguishable from real speakers. Music generation through Suno AI and Stable Audio produces full songs with lyrics from text prompts.

\textbf{Image and Text}

Image generation (Midjourney v6, DALL-E 3, Stable Diffusion XL) produces photorealistic results. Text generation (Claude, GPT-4.5, Gemini) achieves near-human writing quality, can mimic specific styles, and generate fake "eyewitness accounts" of fabricated events.

\textbf{Detection performance: the catastrophic gap.}

\textbf{Human Detection}

The most comprehensive meta-analysis to date~\cite{diel2024deepfakes} examined 56 studies involving 86,155 participants:

Overall accuracy reached only 55.54\% (95\% CI [48.87, 62.10]), with detection rates not significantly above chance (50\%) since confidence intervals crossed the chance threshold. By modality, video achieved 57.31\% [47.80, 66.57], audio 62.08\% [38.23, 83.18], images 53.16\% [42.12, 64.64], and text 52.00\% [37.42, 65.88]. Training interventions improved accuracy to 65.14\% [55.21, 74.46], but this remains far from reliable detection.

Humans fail at detection for several reasons: they focus on wrong cues (blinking, skin texture) that generators have learned to fake, confirmation bias drives perception, cognitive load prevents critical analysis of every piece of media, and resolution improvements have eliminated obvious artifacts.

The AI detection picture is deeply troubling. On training distribution (known techniques), accuracy reaches 95-99\% with low false positive rates and fast processing. On "in the wild" deepfakes~\cite{chandra2025deepfakeeval}, the most comprehensive recent study collected real-world deepfakes from social media and tested state-of-the-art open-source models, revealing catastrophic performance degradation: video models averaged 50\% drop in AUC compared to academic benchmarks, audio models averaged 48\% drop, and image models averaged 45\% drop. Best-performing models on real-world data achieved only 82\% AUC versus 95\%+ on academic datasets, with many models performing barely above chance (53-56\% AUC).

The fundamental problem is that this is an adversarial arms race where generation has structural advantages. Generators see detectors because detection methods must be public to be trusted, so generators train against them. Generators iterate faster because they test offline while detectors wait for real-world deployments. Costs are asymmetric because one evasion technique works broadly while detection must handle all techniques. Economic incentives favor generation (entertainment, advertising) over detection. Training data lags because detectors train on past techniques while generators use current/future techniques.

Academic benchmarks fail to predict real-world performance because they use synthetic, controlled deepfakes with known generation techniques, while real-world deepfakes use latest models, custom techniques, and adversarial adjustments. State-level capabilities (Russian Internet Research Agency, Chinese APT groups, Iranian operations) have demonstrated ability to evade detection for extended periods.

\textbf{The trajectory.}

\textbf{Generation improvement rate:}

\begin{table}[h]
\centering
\begin{tabular}{lllll}
\toprule
Metric &   2020 &   2022 &   2024 &   2025 \\
\midrule
Video quality (FVD) &   250 (obviously fake) &   100 (suspicious artifacts) &   20 (expert scrutiny needed) &   8 (indistinguishable to most) \\
Audio quality (MOS) &   3.2/5.0 (robotic) &   4.0/5.0 (noticeable artifacts) &   4.5/5.0 (subtle issues) &   4.8/5.0 (essentially indistinguishable) \\
Training efficiency &   Voice: 10 min required &   Voice: 30 sec required &   Voice: 5 sec required &   Voice: 3 sec required \\
Cost per minute &   \$50 &   \$5 &   \$1 &   \$0.50 \\
Generation speed &   Minutes &   Seconds &   <10 seconds &   <5 seconds \\
\bottomrule
\end{tabular}
\end{table}


\textbf{Detection deterioration:}

\begin{table}[h]
\centering
\begin{tabular}{lllll}
\toprule
Year &   Generation Quality &   Human Detection &   AI Detection (in-the-wild) &   Gap \\
\midrule
2020 &   Poor &   85\% &   90\% &   Detection ahead \\
2022 &   Moderate &   75\% &   80\% &   Detection ahead \\
2024 &   Good &   60\% &   65\% &   Detection behind \\
2025 &   Excellent &   56\% &   60\% &   Detection failing \\
\bottomrule
\end{tabular}
\end{table}


\textbf{The gap is widening.} Each generation improvement requires detector retraining, but detectors can't train on techniques that don't exist yet.

\textbf{Open-source accessibility:} The performance gap between commercial and open-source generation is closing rapidly (4.52\% gap $\to$ 0.69\% gap in six months). State control of generation is becoming impossible. Anyone with consumer hardware can generate deepfakes.


\subsection{Timeline Analysis}

\textbf{The critical threshold.}

The threshold is crossed when expert detection drops below 60\% accuracy with tools, public detection drops below 25\% accuracy (essentially failed), detection cost exceeds creation cost by 10x or more, and fake content volume creates signal-to-noise collapse.

As of October 2025, expert detection achieves approximately 75\% accuracy with tools (still possible but difficult). Public detection sits at approximately 56\% overall, but \textbf{approximately 25\% for high-quality short clips}---meaning the threshold has already been crossed for the general public on high-quality content. The cost ratio is approximately 5x (approaching threshold), and content volume remains manageable but is growing exponentially.

\textbf{Confidence-calibrated timeline.}

With very high confidence (>90\%), we can state that short-form video (<20 seconds) has crossed the public detectability threshold, open-source models will continue closing the gap with commercial systems, economic incentives favor generation over detection, and generation quality improvement rates will continue in the near term.

With high confidence (>80\%), expert detection will fail for most content within 3-6 years, AI detection degrades catastrophically on real-world content, cryptographic signing will not achieve greater than 50\% adoption within 10 years, and information asymmetry gives generators a permanent advantage.

With medium confidence (50-80\%), generation quality improvement rates will continue long-term (there is no precedent for sudden stops), open-source proliferation will make control impossible, cultural adaptation mechanisms will prove insufficient, and verification will become exponentially (not just linearly) harder.

With low confidence (20-50\%), predictions include the exact timeline for expert detection failure (significant variance exists), when or if feature-length generation becomes viable (2028-2035 range), whether detection can achieve breakthrough improvements, and the effectiveness of regulatory or technical intervention.

\textbf{Uncertainty factors.}

Several factors could delay the threshold: technical barriers we haven't identified, effective regulation limiting development and deployment, breakthroughs in detection technology (such as fundamental physical signatures), social adaptation creating a cultural immune response, and economic disincentives for generation.

Conversely, several factors could accelerate the threshold: AI capability breakthroughs (GPT-5 level models), proliferation to hostile actors, deliberate flooding attacks, loss of trust in verification systems, and recursive improvement (AI improving AI generation).

\textbf{Honest assessment:} Direction is clear (detection losing). Timeline has uncertainty (3-6 year range). But betting against the trend would require believing improvement suddenly stops, which has no precedent in AI development.

\textbf{Timeline sensitivity analysis.}

To make our projections more rigorous, we model three scenarios based on different improvement rates:

\textbf{Baseline Projection (Current Trajectory):}

This scenario assumes detection accuracy improves at 5\% annually (current trend), generation quality improves at 15\% annually (current trend), the gap widens at 10\% annually, and current state shows human detection at 55.54\% and expert detection at approximately 75\%.

Under these assumptions, expert detection falls below 60\% in 3-4 years (2028-2029), public detection falls below 25\% for all content in 5-6 years (2030-2031), and cost ratio exceeds 10x in 4-5 years (2029-2030). This projection has high confidence (>80\%) as it extrapolates current demonstrated trends.

\textbf{Optimistic Scenario (Detection Breakthrough):}

This scenario assumes detection accuracy improves at 20\% annually (requiring a major breakthrough), generation quality improves at 15\% annually (continuing current trends), the gap narrows at 5\% annually, and a breakthrough occurs in the next 1-2 years.

Under these assumptions, expert detection maintains above 60\% for 8-12 years (2033-2037), public detection stabilizes at approximately 40\% beyond 10 years, and cost ratio stays below 10x for 7-10 years. This projection has low confidence (<30\%) as it requires unprecedented detection advancement with no historical precedent.

This scenario would require fundamental physical signatures being discovered that generators cannot spoof, quantum-based verification deployed at scale, international cooperation enforcing generation limits (extremely unlikely), or an AI development plateau (no historical precedent).

\textbf{Pessimistic Scenario (Generation Acceleration):}

This scenario assumes detection accuracy improves at 5\% annually (current trend continues), generation quality improves at 25\% annually (GPT-5 level advancement), the gap widens at 20\% annually, and a major AI capability jump occurs in the next 1-2 years.

Under these assumptions, expert detection falls below 60\% in 1.5-2.5 years (late 2026-late 2027), public detection falls below 25\% for most content in 2-3 years (2027-2028), and cost ratio exceeds 10x in 2-3 years (2027-2028). This projection has medium confidence (40-60\%) as it is plausible given AI development trajectory and economic incentives.

This scenario would be triggered by GPT-5 or equivalent being released with a major capability jump, open-source models reaching parity with the best commercial systems (already happening with the 0.69\% gap), recursive self-improvement in generation models, or state actors deliberately flooding the information space.

\textbf{Current Indicators:}

\begin{table}[h]
\centering
\begin{tabular}{lllll}
\toprule
Metric &   Baseline &   Optimistic &   Pessimistic &   Current Trend \\
\midrule
Open-source gap closing &   10\% annually &   5\% annually &   15\% annually &   \textbf{15\% (4.52\% $\to$ 0.69\% in 6 months)} $\checkmark$ Pessimistic \\
Human detection accuracy &   Stable ~55\% &   Improves to 65\% &   Declines to 45\% &   \textbf{Declining (55.54\% and falling)} $\checkmark$ Pessimistic \\
AI detection real-world &   Stable ~60\% &   Improves to 75\% &   Declines to 50\% &   \textbf{Declining (45-50\% drop from academic)} $\checkmark$ Pessimistic \\
Investment ratio (gen/det) &   1000:1 &   100:1 &   5000:1 &   \textbf{\textasciitilde1000:1 and widening} $\checkmark$ Baseline-Pessimistic \\
Cost ratio (verify/create) &   $5x \to 10x$ &   $5x \to 3x$ &   $5x \to 20x$ &   \textbf{Currently \textasciitilde5x, growing} $\checkmark$ Baseline \\
\bottomrule
\end{tabular}
\end{table}


\textbf{Current trajectory most consistent with baseline-to-pessimistic range.}

\textbf{Probability Assessment:}

Based on current indicators:
- Pessimistic scenario: \textbf{40\% probability}
- Baseline scenario: \textbf{50\% probability}
- Optimistic scenario: \textbf{10\% probability}

\textbf{Expected timeline to threshold} (probability-weighted):
- 50th percentile: \textbf{3-4 years} (2028-2029)
- 75th percentile: \textbf{2-3 years} (2027-2028)
- 90th percentile: \textbf{1.5-2 years} (late 2026-2027)

\textbf{Decision implications:}

Even under optimistic scenario (8-12 years), examination requires years and must begin immediately. Under baseline/pessimistic scenarios, window is critically short.

The asymmetry of risk remains total: if we act on the pessimistic timeline and it turns out optimistic, there is no harm and extra time is a bonus; if we act on the optimistic timeline and it turns out pessimistic, the result is catastrophic and we miss the window entirely.

The rational strategy is to act on the pessimistic timeline (1.5-2.5 years). Even if its probability is only 40\%, the cost of being wrong is infinite.


\subsection{Why Countermeasures Will Likely Fail}

\textbf{Cryptographic content authentication.}

\textbf{The proposal:} Sign content at capture with unforgeable cryptographic signatures. Chain of custody maintained through editing. Unsigned content treated as untrusted.

\textbf{Technical soundness:} The cryptography is mathematically robust. This could theoretically work.

\textbf{Adoption barriers make success unlikely.}

Hardware requirements present the first major barrier: universal hardware replacement would be needed for every camera and microphone globally, legacy devices would remain unsigned (everything manufactured before implementation), costs would reach trillions of dollars globally, and full adoption would take decades.

Technical vulnerabilities compound this problem. State actors can extract keys through hardware compromise, supply chain attacks can compromise devices at manufacture, key management raises the question of who controls root certificates, and side-channel attacks make keys extractable through various methods.

Governance problems add another layer. International coordination would be required despite divergent state interests, states can mandate backdoors, authoritarian regimes can control key distribution, and corporations would control the signing infrastructure.

\textbf{The bootstrapping problem:} During the transition period (which could last decades), the information commons is already poisoned. You can't coordinate a global transition when you can't trust information about the transition itself.

\textbf{Confidence assessment:} Very low confidence (<20\%) this achieves >80\% adoption within 20 years.

\textbf{Blockchain provenance tracking.}

\textbf{The proposal:} Record content creation and modifications on blockchain for immutable audit trail.

\textbf{Fundamental flaw:} Blockchain verifies the record, not the content. "Garbage in, garbage out." It can record that a deepfake was created at time T but cannot verify content authenticity at capture, doesn't solve the initial verification problem, and provides no mechanism to remove false information once recorded.

\textbf{Confidence assessment:} This doesn't solve the verification problem at all.

\textbf{AI detection improvements.}

\textbf{Why detection is mathematically losing:}

If a generator reaches perfection (statistically indistinguishable from real), detection becomes theoretically impossible. We're approaching this limit. Best generators already fool expert humans. Detection relies on generator imperfections. As imperfections vanish, detection fails.

Resource asymmetry heavily favors generation: billions are invested in generation versus millions in detection (a 1000:1 funding disparity), generation has positive economic value through entertainment, advertising, and productivity while detection is a cost center with no revenue, and market forces structurally favor generation.

The adversarial advantage compounds this asymmetry. Generators can train specifically to evade detection, detection methods must be public to be trusted, generators iterate faster through offline testing versus deployment cycles, and one evasion technique defeats many detectors.

\textbf{Confidence assessment:} Low confidence (<30\%) that detection keeps pace with generation over 5+ years.

\textbf{Social/cultural adaptation.}

\textbf{The proposal:} Society develops cultural norms to handle synthetic media through default skepticism, trust networks, reduced reliance on media evidence, and new social technologies.

\textbf{Why this may be insufficient:}

\textbf{Coordination requires shared reality:} If everyone has different "truth," coordination collapses. Extreme skepticism prevents coordination as much as credulity does.

\textbf{Speed mismatch:} Cultural evolution takes generations. Synthetic media is improving in years. Speed mismatch creates crisis period.

\textbf{Historical precedent:} Previous media revolutions (printing, radio, TV, internet) took decades to adapt. We don't have decades. Each previous revolution eventually stabilized, but the transition periods were characterized by massive social disruption.

\textbf{Confidence assessment:} Medium confidence (40-60\%) that cultural adaptation provides *some* mitigation, but low confidence it prevents coordination collapse.


\subsection{Current Real-World Impact}

\textbf{Documented harms (October 2025).}

In the political sphere, documented harms include fabricated politician statements during elections in multiple countries, false video "evidence" of corruption, synthetic "endorsements" from respected figures, and a growing problem across both democracies and autocracies.

Financial fraud has escalated dramatically. CEO voice deepfakes authorizing wire transfers have caused \$35M losses in one documented case, synthetic video meetings enable social engineering attacks, fake product reviews and testimonials operate at scale, and stock manipulation occurs through fabricated news.

Social manipulation takes particularly harmful forms: non-consensual intimate imagery predominantly targeting women, fabricated evidence in legal disputes, synthetic personas spreading disinformation, and harassment through impersonation.

Perhaps most insidiously, the erosion of trust creates a "liar's dividend." Real videos are dismissed as deepfakes, footage from conflict zones cannot be verified, politicians pre-emptively claim videos are fake, and general paralysis affects information evaluation.

\textbf{The qualitative shift.}

From 2020-2023, deepfakes were novelties---expensive and obvious. In 2024-2025, deepfakes became cheap, accessible, and convincing. By 2026 and beyond (projected), they will be indistinguishable at scale.

The question has shifted from "can it be done?" to "can it be detected?" to "can anything be trusted?"


\subsection{Implications for Voluntary Coordination}

\textbf{Why the window is closing.}

Now (October 2025), truth can still be verified with effort as experts can distinguish most content, expert tools still work on most content with careful analysis, obvious deepfakes remain identifiable, and institutions haven't fully adapted to the threat.

Soon (2-5 years), routine verification becomes exponentially harder, expert tools fail on most content, no reliable way exists to distinguish real from fake for most people, and trust in all media collapses.

After the threshold, coordination requires trust, trust requires verification, verification becomes impossible, and therefore coordination collapses.

\textbf{Why this matters for voluntary coordination.}

Voluntary coordination requires:

\textbf{Verifying traditions against source texts} $\to$ After threshold: source texts can be fabricated, cannot verify which interpretations are accurate

\textbf{Seeing institutional betrayals clearly} $\to$ After threshold: betrayals can be hidden, evidence dismissed as "deepfakes," whistleblowers discredited

\textbf{Coordinating around observable truth} $\to$ After threshold: truth becomes unknowable, no shared reality to coordinate around

\textbf{Building trust networks based on verification} $\to$ After threshold: impossible to bootstrap trust, cannot verify anyone's identity or claims

\textbf{The asymmetry of risk.}

If the threshold is 10 years away, we have more time than expected, early action still benefits from the extra time, there is no cost to acting sooner since examination is still valuable, and preparation helps even if the timeline is longer.

If the threshold is 2 years away, we have much less time than hoped, delay is catastrophic, acting immediately is essential, and there is no time for preparation.

The rational choice is to act as if the aggressive timeline is correct. The cost of being wrong is asymmetric: if wrong about a long timeline and we act unnecessarily early, the cost is minimal since examination is still valuable; if wrong about a short timeline and we delay when time is critical, the cost is catastrophic and results in inability to coordinate for survival.

Decision theory requires expected value maximization, which dictates acting on the aggressive timeline.


\subsection{Uncertainty and Falsification}

\textbf{What we know vs. what we don't.}

With very high confidence (>90\%), short-form video has crossed the public detection threshold, open-source is closing the gap with commercial models, economic incentives structurally favor generation, detection degrades on real-world content, and generation quality is improving rapidly.

With high confidence (>80\%), expert detection will fail for most content within 3-6 years, cryptographic signing won't achieve critical mass, information asymmetry gives generators a permanent advantage, and cultural adaptation will prove insufficient.

With medium confidence (50-80\%), verification becomes exponentially (not just linearly) harder, feature-length generation becomes viable by 2030-2035, countermeasures fail to prevent threshold crossing, and timeline estimates have $\pm$2 year accuracy.

With low confidence (20-50\%), we cannot precisely predict the exact timeline for various milestones, the effectiveness of unknown countermeasures, the rate of cultural adaptation, or whether breakthrough detection methods are possible.

\textbf{Falsification criteria.}

\textbf{We're wrong if:}

\textbf{Prediction 1:} Detection accuracy improves faster than generation quality for 3+ consecutive years. Current status: generation is improving faster (gap widening). Metric to track: human detection accuracy and AI detection AUC on real-world content.

\textbf{Prediction 2:} Cryptographic content authentication achieves greater than 80\% market adoption by 2030. Current status: less than 1\% adoption with no clear path to deployment. Metric to track: percentage of devices with signing capability.

\textbf{Prediction 3:} Verification cost decreases relative to generation cost. Current status: cost ratio is approximately 5x and growing. Metric to track: Cost(verification)/Cost(generation).

\textbf{Prediction 4:} A fundamentally new detection approach emerges that generators cannot evade. Current status: no such approach has been identified. Metric to track: detection accuracy on adversarially-generated content.

These metrics can be tracked through human detection accuracy on latest models (currently 55.54\%), AI detection AUC on real-world deepfakes (currently approximately 60\%), open-source versus commercial performance gap (currently 0.69\%), cost ratio of verification to generation (currently approximately 5x), and cryptographic signing adoption rate (currently approximately 0\%).

\textbf{Comparison to previous failed predictions.}

\textbf{Why this isn't like Malthus:}

Malthus predicted population collapse based on fixed technology. He was logically sound given his assumptions, but technology improved (Green Revolution, mechanization, etc.). His error was assuming technology was static.

Our prediction explicitly accounts for technology improvement: we predict generation improves faster than detection, which \emph{is} the technology improvement. Our claim concerns the relative trajectory, not absolute capability, and falsification requires detection improving faster than generation (which is testable).

The key difference from Malthus is that he assumed technology was static and was proved wrong. We assume technology improves and base predictions on which technology (generation versus detection) has structural advantages.

Similar failed predictions include "end of history," various "singularity" predictions with precise dates, and Y2K catastrophe predictions. These failed because they underestimated human adaptation, overestimated single-factor importance, ignored feedback mechanisms, and made overly precise predictions.

Our prediction differs in several ways: we explicitly model the adversarial arms race, account for economic and structural advantages, provide ranges rather than precise dates, have empirical evidence of the current trajectory, and specify falsification criteria.

However, we could still be wrong. Perhaps there will be a detection breakthrough we haven't envisioned, cultural adaptation proves faster than expected, regulatory coordination succeeds unexpectedly, or economic incentives shift dramatically.

The difference is: we've made our assumptions explicit, provided falsification criteria, and shown why the trajectory is structurally determined.

\textbf{Unknown unknowns.}

\textbf{What could we be missing?}

\textbf{Quantum-based verification methods:} Currently theoretical, no clear path to deployment, but might provide unforgeable signatures based on quantum effects.

\textbf{Emergent social technologies:} New coordination mechanisms we haven't conceived that work without verification.

\textbf{AI capability plateaus:} No historical precedent, but theoretically possible that AI development slows dramatically.

\textbf{Cultural adaptation we haven't envisioned:} Humans are creative. Maybe we develop coordination mechanisms that work despite verification failure.

\textbf{Regulatory breakthroughs:} International coordination on AI development restrictions. Low probability given state competition dynamics.

The honest assessment is that we don't know what we don't know. The best we can do is make assumptions explicit, provide falsification criteria, track metrics in real-time, update as evidence changes, and act on the best available evidence.

\textbf{Why uncertainty doesn't change urgency.}

\textbf{The asymmetry again:}

Even with significant uncertainty about exact timeline:

\begin{table}[h]
\centering
\begin{tabular}{lll}
\toprule
Timeline Scenario &   Probability &   Action Required \\
\midrule
Threshold in 2 years &   20\% &   Act immediately \\
Threshold in 4 years &   50\% &   Act immediately \\
Threshold in 6 years &   20\% &   Act immediately \\
Threshold in 10+ years &   10\% &   Act immediately \\
\bottomrule
\end{tabular}
\end{table}


All scenarios require immediate action because examination takes time and cannot be rushed, waiting for certainty means it's already too late, there is no cost to acting early if the timeline is longer, and the cost of acting late is catastrophic if the timeline is shorter.

\textbf{Expected value calculation:}

Let $t$ = actual time to threshold, $p(t)$ = probability distribution over $t$.

Expected value of acting now:
$E[V_{now}] = \int_0^{\infty} V(t) \cdot p(t) \, dt$

Expected value of waiting:
$E[V_{wait}] = \int_0^{t_{wait}} 0 \cdot p(t) \, dt + \int_{t_{wait}}^{\infty} V(t - t_{wait}) \cdot p(t) \, dt$

Since $V(t - t_{wait}) < V(t)$ (less time available), and there's probability mass in $[0, t_{wait}]$ that's lost entirely:

$E[V_{now}] > E[V_{wait}]$

\textbf{Translation:} Acting now is superior regardless of uncertainty about exact timeline.


\textbf{References and citation quality.}

Full references are provided in the bibliography at the end of this document. Key sources include:

\textbf{Peer-reviewed sources (high confidence):}~\cite{diel2024deepfakes,somoray2025deepfake,groh2022deepfake,abbasi2025comprehensive,bhandarkawthekar2025rlnet}

\textbf{Preprint/arXiv (medium-high confidence):}~\cite{chandra2025deepfakeeval}

\textbf{Industry documentation (medium confidence):}~\cite{openai2025sora2,openai2025sora2systemcard}

\textbf{Journalistic coverage (lower confidence for technical claims):}~\cite{cjr2025deepfake}

\textbf{Citation quality assessment.}

High confidence sources come from peer-reviewed, reputable journals including \emph{Computers in Human Behavior}, \emph{Human Behavior and Emerging Technologies}, \emph{PNAS}, \emph{Applied Sciences}, and \emph{Frontiers} journals. These feature transparent and reproducible methodology with independent verification possible.

Medium confidence sources include industry documentation and preprints: Deepfake-Eval-2024 (arXiv preprint with sound methodology but not yet peer-reviewed) and OpenAI technical documentation (industry source without independent verification).

Lower confidence sources include journalistic coverage: media coverage of capabilities that reports on claims without independent testing, and feature-length movie claims based on social media posts without technical roadmaps.

Critical gaps in available evidence include limited independent benchmarking of commercial systems, no peer-reviewed papers on some claimed capabilities, and timeline predictions that lack formal uncertainty quantification in the source material.


\subsection{Conclusion}

The evidence establishes several facts with very high confidence: current generation capabilities have crossed the public detectability threshold for short-form content, human detection has failed at 55.54\% overall accuracy (barely above chance), AI detection degrades catastrophically on real-world content (45-50\% performance drop), open-source proliferation makes control impossible, economic incentives strongly favor generation over detection, and the gap is widening rather than closing.

With high confidence, we can state that expert detection will fail for most content within 3-6 years, cryptographic countermeasures face insurmountable adoption barriers, cultural adaptation is too slow to prevent a crisis period, and verification will become exponentially harder.

What remains uncertain includes the exact timeline to expert detection failure (range: 3-6 years), whether detection can achieve breakthrough improvement, effectiveness of cultural adaptation, whether regulatory intervention can meaningfully slow development, and the feature-length generation timeline (2028-2035 range, high variance).

The direction is certain; the timeline is uncertain. But uncertainty about timeline doesn't change the fundamental trajectory.

Voluntary coordination requires verifiable truth. Within years, routine verification becomes exponentially harder or impossible. The window for building coordination systems based on verifiable reality is closing.

\textbf{You can examine source texts, verify institutional betrayals, and coordinate around observable truth NOW while verification is still possible.} After the threshold, these foundations become unavailable. The examination must happen while truth remains knowable.

Given timeline uncertainty, how should we act? The conservative estimate of 6 years to threshold provides some breathing room but still requires immediate action because examination takes years, with no room for delay. The aggressive estimate of 2-3 years to threshold requires immediate action with no time for delay or preparation, meaning examination must begin now. The rational strategy is to act on the aggressive timeline. If the conservative estimate is correct and we act aggressively, there is no harm and extra time is a bonus. If the aggressive estimate is correct and we delay, the outcome is catastrophic and we miss the window entirely. Expected value maximization requires acting on the short timeline.

This is not speculation but documented technological reality unfolding in real-time: human detection at 55.54\% (published meta-analysis), AI detection degradation of 45-50\% drop (peer-reviewed studies), open-source gap decreasing from 4.52\% to 0.69\% in 6 months (documented), and economic incentives showing 1000:1 funding disparity (observable). The evidence is clear. The trajectory is established. The window is closing.

You can examine while truth is verifiable, or wait until it's impossible. The choice is yours, but the window won't wait for you to decide.


\textbf{Notation and terminology reference.}

\begin{table}[h]
\centering
\begin{tabular}{ll}
\toprule
Term &   Definition \\
\midrule
FVD &   Fr√©chet Video Distance (lower is better; measures video quality) \\
MOS &   Mean Opinion Score (scale of 1-5 for perceived quality) \\
AUC &   Area Under Curve (detection accuracy metric; 1.0 = perfect) \\
Deepfake &   Synthetic media created by AI to impersonate real people/events \\
Detection threshold &   Point where detection accuracy falls below useful level (~60\% for experts, ~25\% for public) \\
Generation &   Creating synthetic media (video, audio, image, text) \\
Detection &   Identifying synthetic media as fake \\
Open-source &   Publicly available code/models anyone can use \\
Commercial &   Proprietary systems available only through companies \\
Real-world performance &   Accuracy on actual deepfakes from social media (vs. academic benchmarks) \\
Academic benchmarks &   Controlled test datasets with known generation techniques \\
\bottomrule
\end{tabular}
\end{table}



\textbf{Final assessment.}

This appendix establishes the current state (public detection has failed and expert detection is struggling), the trajectory (gap widening as generation improves faster than detection), the timeline (3-6 years with high confidence until expert detection fails), the likely failure of countermeasures to prevent threshold crossing, the implications (the window for verification-based coordination is closing), and the required action (examine NOW while truth remains verifiable).

The evidence is conclusive. The stakes are absolute. The window is closing.

