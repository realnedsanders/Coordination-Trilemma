\section{Methodology}
\label{app:e}

This appendix details the empirical and computational methods used to validate the theoretical claims in this paper. We describe our historical case selection criteria, data sources, coding schemes, and statistical approaches, as well as the computational models and their parameterization.

\subsection{Historical Case Study Methodology}

\heading{Case selection criteria}

We systematically selected historical coordination systems for analysis based on the following criteria: population scale exceeding $10^6$ agents, documented governance and enforcement structures, sufficient historical record to assess corruption and stability over time (minimum 50 years of data), and geographic and temporal diversity to avoid selection bias.

\subheading{Selected cases}

\begin{itemize}
    \item \emph{Roman Empire} (27 BCE--476 CE): Population $\sim$60--70M at peak. Extensive records of corruption dynamics, enforcement hierarchies, and collapse mechanisms.
    \item \emph{Han Dynasty} (206 BCE--220 CE): Population $\sim$55M. Parallel cycle of centralization, corruption, and fragmentation documented in dynastic histories.
    \item \emph{Byzantine Empire} (330--1453 CE): Population $\sim$26M at peak. Longest continuous imperial system with detailed administrative records.
    \item \emph{Ming Dynasty} (1368--1644 CE): Population $\sim$160M. Well-documented corruption cascade leading to collapse.
    \item \emph{British Colonial System} (1757--1947 CE): Population $\sim$400M administered. Modern administrative records with quantifiable corruption metrics.
    \item \emph{Soviet Union} (1922--1991 CE): Population $\sim$290M at dissolution. Systematic enforcement failure with extensive archival access post-1991.
\end{itemize}

\subheading{Exclusion criteria}

Cases were excluded if: (a) population scale below $10^6$, preventing comparison with theoretical predictions; (b) insufficient primary source documentation (<3 independent sources); (c) time horizon under 50 years; or (d) no systematic enforcement hierarchy present (e.g., stateless societies).

\heading{Data sources}

Primary sources include Tainter's collapse database, Turchin and Nefedov's secular cycles data, Acemoglu and Robinson's institutional datasets, and original archival research for specific cases.

\subheading{Primary databases}

\begin{itemize}
    \item \emph{Seshat: Global History Databank} -- Standardized variables for 400+ polities across 10,000 years. Used for: population scale, territorial extent, administrative hierarchy depth.
    \item \emph{V-Dem Dataset v13} -- Expert-coded measures of governance quality for 202 polities since 1789. Used for: corruption indicators, rule of law metrics, enforcement effectiveness.
    \item \emph{Polity IV/V Project} -- Political regime characteristics (1800--present). Used for: institutional stability, regime transitions.
    \item \emph{Correlates of War Project} -- State system membership and conflict data. Used for: collapse events, territorial fragmentation.
\end{itemize}

\subheading{Secondary sources}

For pre-modern cases, we rely on synthetic works: Tainter's \emph{Collapse of Complex Societies} (1988) for Roman and Han data; Turchin and Nefedov's \emph{Secular Cycles} (2009) for cyclical dynamics; Bang and Scheidel's \emph{State in the Ancient World} (2013) for administrative structures. Soviet archival data from Kuromiya and Khlevniuk's work on Politburo records; British colonial data from Imperial Gazetteer volumes and India Office Records.

\heading{Coding scheme}

We coded each case on the following variables:

\begin{table}[h]
\centering
\small
\begin{tabular}{lllp{5cm}}
\toprule
\textbf{Variable} & \textbf{Type} & \textbf{Scale} & \textbf{Definition} \\
\midrule
Population scale & Continuous & Log & Peak administered population \\
Hierarchy depth & Ordinal & 1--6 & Administrative levels (1=flat, 6=deep) \\
Corruption index & Ordinal & 0--10 & Expert-coded endemic corruption \\
Enforcement effectiveness & Continuous & 0--1 & Compliance rate where measured \\
System longevity & Continuous & Years & Time to collapse/major fragmentation \\
Collapse mode & Categorical & --- & Internal corruption, external conquest, both \\
Recovery & Binary & 0/1 & System reconstituted within 50 years \\
\bottomrule
\end{tabular}
\end{table}

\subheading{Coding procedure}

Two researchers independently coded each variable for all cases. Disagreements were resolved by discussion and, if necessary, adjudication by a third coder. For ordinal variables, we achieved inter-rater reliability of $\kappa = 0.78$ (Cohen's kappa, substantial agreement). Continuous variables showed ICC(2,2) = 0.89 (excellent reliability). All coding decisions are documented in the supplementary materials with justifications and source citations.

\heading{Limitations}

Historical data has inherent limitations including survivorship bias (we only observe systems that left records), measurement error in corruption indicators, and difficulty establishing counterfactuals.


\subsection{Computational Model Specifications}

\heading{Agent-based corruption dynamics model}

The corruption dynamics model simulates enforcer behavior over time in hierarchical systems. Agents are characterized by integrity motivation $M_{\text{integrity}}$, extraction opportunity $U_e$, and detection probability $P_{\text{detection}}$.

\subheading{Agent decision rule}

Each step, agent $i$ faces extraction opportunity $U_e$ drawn from $\mathcal{N}(\mu_e, \sigma_e)$ scaled by power level $(1 + (1 - O_i))$ where $O_i$ is oversight level. Agent extracts iff:
\[
U_e > C_d \cdot P_{\text{detection}} + M_{\text{integrity}}
\]
where $C_d$ is detection cost and $P_{\text{detection}} = P_{\text{base}} \cdot O_i$.

\subheading{Oversight structure}

Hierarchical structure assigns oversight levels: top 10\% receive $O_i = 0.1$; next 20\% receive $O_i = 0.4$; middle 20\% receive $O_i = 0.6$; bottom 50\% receive $O_i = 0.9$. This captures the enforcement regress problem: top enforcers have minimal oversight.

\subheading{Dynamic mechanisms}

\begin{itemize}
    \item \emph{Integrity decay}: Upon extraction, $M_{\text{integrity}} \gets M_{\text{integrity}} \cdot (1 - \delta_d)$ with $\delta_d = 0.05$
    \item \emph{Corruption contagion}: Extracting agents reduce nearby agents' integrity by factor $(1 - \delta_c)$ with $\delta_c = 0.02$
    \item \emph{Integrity reinforcement} (optional): Honest behavior in corrupt environment increases $M_{\text{integrity}}$
\end{itemize}

\subheading{Default parameters}

\begin{table}[h]
\centering
\small
\begin{tabular}{llll}
\toprule
\textbf{Parameter} & \textbf{Symbol} & \textbf{Default} & \textbf{Range tested} \\
\midrule
Number of enforcers & $N$ & 100 & 50--500 \\
Integrity mean & $\mu_M$ & 5.0 & 1.0--10.0 \\
Integrity std & $\sigma_M$ & 2.0 & 0.5--4.0 \\
Extraction mean & $\mu_e$ & 3.0 & 1.0--8.0 \\
Extraction std & $\sigma_e$ & 1.5 & 0.5--3.0 \\
Base detection prob & $P_{\text{base}}$ & 0.3 & 0.1--0.9 \\
Detection cost & $C_d$ & 10.0 & 5.0--20.0 \\
Time steps & $T$ & 200 & 100--500 \\
\bottomrule
\end{tabular}
\end{table}

\subheading{Validation}

Model validated against: (1) historical corruption rates in well-documented cases (Roman provincial administration, Soviet party apparatus); (2) theoretical predictions from enforcement regress analysis; (3) sensitivity analysis confirming robust convergence to high corruption across parameter space.

\heading{Cooperation threshold model}

This model explores the critical mass dynamics of voluntary coordination, testing the relationship between transformation proportion $\theta$, intrinsic motivation distribution, and cooperation stability.

\subheading{Agent decision rule}

Each step, agent $i$ cooperates iff:
\[
M_i + \eta_i > c - \beta \cdot \theta
\]
where $M_i$ is intrinsic motivation, $\eta_i \sim \mathcal{N}(0, \sigma_\eta)$ is decision noise, $c$ is cooperation cost, $\beta$ is benefit multiplier, and $\theta = k/N$ is current cooperation rate.

\subheading{Network effects}

When enabled, effective motivation includes social proof: $M_i^{\text{eff}} = M_i + \gamma \cdot \theta$ where $\gamma$ is network strength parameter.

\subheading{Theoretical critical mass}

From Theorem 4.2, the critical threshold is:
\[
\theta_{\text{crit}} = \frac{c}{\beta + \bar{M}}
\]
where $\bar{M}$ is mean motivation. With default parameters ($c = 1.0$, $\beta = 2.0$, $\bar{M} = 0.5$), we obtain $\theta_{\text{crit}} = 0.40$.

\subheading{Motivation dynamics}

Motivation evolves based on outcomes:
\begin{itemize}
    \item Cooperating in cooperative environment ($\theta > 0.5$): $M_i \gets \min(M_i \cdot (1 + \delta_r), 2M_i^0)$
    \item Cooperating when rare ($\theta < 0.5$): $M_i \gets M_i \cdot (1 - \delta_d)$
    \item Defecting: $M_i \gets 0.99 M_i + 0.01 M_i^0$ (gradual return to baseline)
\end{itemize}

\subheading{Default parameters}

\begin{table}[h]
\centering
\small
\begin{tabular}{llll}
\toprule
\textbf{Parameter} & \textbf{Symbol} & \textbf{Default} & \textbf{Range tested} \\
\midrule
Number of agents & $N$ & 1000 & 100--10000 \\
Cooperation cost & $c$ & 1.0 & 0.5--2.0 \\
Benefit multiplier & $\beta$ & 2.0 & 1.0--4.0 \\
Motivation mean & $\bar{M}$ & 0.5 & 0.1--1.0 \\
Motivation std & $\sigma_M$ & 0.3 & 0.1--0.5 \\
Network strength & $\gamma$ & 0.5 & 0.0--1.0 \\
Decision noise & $\sigma_\eta$ & 0.1 & 0.0--0.3 \\
Reinforcement rate & $\delta_r$ & 0.02 & 0.0--0.1 \\
Discouragement rate & $\delta_d$ & 0.01 & 0.0--0.05 \\
\bottomrule
\end{tabular}
\end{table}

\heading{Monte Carlo cycle simulations}

We simulate the corruption-to-TCS cycle dynamics using Monte Carlo methods to generate probability distributions over outcomes and timelines.

\subheading{State space}

The simulation models four states from Theorem 3.2:
\begin{itemize}
    \item $S_C$: Corruption phase (human enforcement, initial state)
    \item $S_{\text{TCS-H}}$: TCS with human controllers
    \item $S_{\text{TCS-AI}}$: TCS with autonomous AI control
    \item $S_E$: Extinction/enslavement (absorbing state)
\end{itemize}

\subheading{Transition dynamics}

From $S_C$: With probability $P_{\text{TCS}}$, transition to TCS; otherwise restart corruption cycle. If transitioning to TCS, probability $P_{\text{AI}}$ determines AI vs human control.

From $S_{\text{TCS-H}}$: Controllers eventually corrupt (Theorem 2.1); deterministic return to $S_C$.

From $S_{\text{TCS-AI}}$: With probability $P_{\text{align}}$, alignment succeeds (return to $S_C$); otherwise transition to $S_E$.

Technological progress: $P_{\text{AI}} \gets \min(0.99, P_{\text{AI}} \cdot (1 + \delta_g))$ after each cycle.

\subheading{Cycle duration}

Each cycle duration sampled from $\mathcal{N}(\mu_{\text{cycle}}, \sigma_{\text{cycle}})$, truncated to ensure positivity.

\subheading{Default parameters}

\begin{table}[h]
\centering
\small
\begin{tabular}{llll}
\toprule
\textbf{Parameter} & \textbf{Symbol} & \textbf{Default} & \textbf{Range tested} \\
\midrule
Number of simulations & $n$ & 100,000 & --- \\
Initial AI probability & $P_{\text{AI}}$ & 0.05 & 0.01--0.5 \\
TCS transition prob & $P_{\text{TCS}}$ & 0.8 & 0.5--1.0 \\
Alignment probability & $P_{\text{align}}$ & 0.0 & 0.0--0.99 \\
Cycle duration mean & $\mu_{\text{cycle}}$ & 50 years & 20--100 \\
Cycle duration std & $\sigma_{\text{cycle}}$ & 20 years & 5--40 \\
AI growth rate & $\delta_g$ & 0.1 & 0.0--0.3 \\
Maximum time & $T_{\max}$ & 1000 years & --- \\
\bottomrule
\end{tabular}
\end{table}

\subheading{Convergence diagnostics}

With $n = 100{,}000$ simulations, Monte Carlo standard error for extinction probability is $\text{SE} = \sqrt{p(1-p)/n} \approx 0.001$ for $p \approx 0.95$. Median time estimates converge with relative error $<1\%$. Parallel execution using 8+ CPU cores; typical runtime 1--2 seconds for 100K simulations.


\subsection{Statistical Methods}

\heading{Survival analysis}

We use Kaplan-Meier estimation and Cox proportional hazards models to analyze coordination system longevity as a function of scale and institutional features.

\subheading{Kaplan-Meier estimation}

Non-parametric survival curves estimated for coordination system longevity. Systems that persist to present are right-censored. Standard Greenwood confidence intervals computed with 95\% coverage.

\subheading{Cox proportional hazards model}

We model hazard of collapse as:
\[
h(t) = h_0(t) \exp(\beta_1 \log N + \beta_2 D + \beta_3 C + \beta_4 E)
\]
where $N$ is population scale, $D$ is hierarchy depth, $C$ is corruption index, and $E$ is enforcement effectiveness.

\subheading{Model diagnostics}

\begin{itemize}
    \item \emph{Proportional hazards assumption}: Tested via Schoenfeld residuals; satisfied for all covariates ($p > 0.05$)
    \item \emph{Linearity}: Log-linearity of continuous covariates verified via Martingale residuals
    \item \emph{Influential observations}: dfbeta analysis identified no high-leverage cases
    \item \emph{Goodness of fit}: Concordance index C = 0.72 (moderate discrimination)
\end{itemize}

\subheading{Key findings}

Hazard ratios (95\% CI): log-scale HR = 1.23 (1.08--1.41); hierarchy depth HR = 1.15 (1.02--1.31); corruption index HR = 1.42 (1.21--1.67). Results confirm theoretical predictions: larger scale and higher corruption increase collapse hazard.

\heading{Bayesian uncertainty quantification}

Timeline predictions incorporate Bayesian methods to properly quantify uncertainty and update as evidence accumulates.

\subheading{Prior specifications}

For timeline predictions:
\begin{itemize}
    \item Cycle duration: $\mu_{\text{cycle}} \sim \text{Gamma}(2.5, 0.05)$, centered at 50 years with moderate uncertainty
    \item Initial AI probability: $P_{\text{AI}} \sim \text{Beta}(2, 38)$, centered at 0.05 with right skew
    \item AI growth rate: $\delta_g \sim \text{Beta}(2, 18)$, centered at 0.1
    \item Alignment probability: $P_{\text{align}} \sim \text{Beta}(1, 9)$, skeptical prior centered at 0.1
\end{itemize}

Priors chosen to be weakly informative: they regularize inference but are dominated by likelihood with moderate data.

\subheading{Posterior computation}

Monte Carlo integration over prior distributions. For each of 100,000 trajectory simulations, parameters drawn from priors, yielding posterior predictive distribution for extinction time.

\subheading{Sensitivity analysis}

Results robust to reasonable prior variations. Doubling prior variance for all parameters changes median extinction estimate by $<$15\%. Key driver is structural assumption of corruption inevitability (Theorem 2.1), not specific prior choices.

\subheading{Posterior diagnostics}

Posterior predictive checks confirm model captures observed historical cycle durations (posterior predictive $p$-value = 0.34). No evidence of model misspecification in cycle timing. Limitations: future AI capabilities are fundamentally uncertain, so long-horizon predictions should be interpreted as conditional on model assumptions.


\subsection{Reproducibility}

All computational analyses are fully reproducible. Code, data, and instructions are available in the supplementary materials repository. Random seeds are fixed for all stochastic simulations.

\subheading{Repository}

Code and data available at: \url{https://github.com/realnedsanders/Coordination-Trilemma}

The repository includes:
\begin{itemize}
    \item \texttt{models/} -- All computational models (Python ABMs, Go simulations)
    \item \texttt{data/} -- Historical datasets and simulation outputs
    \item \texttt{figures/} -- Generated visualizations
    \item \texttt{src/tex/} -- \LaTeX{} source for this paper
\end{itemize}

\subheading{Software environment}

\begin{table}[h]
\centering
\small
\begin{tabular}{ll}
\toprule
\textbf{Component} & \textbf{Version} \\
\midrule
Python & 3.11+ \\
Mesa (ABM framework) & 3.0+ \\
NumPy & 1.24+ \\
Matplotlib & 3.7+ \\
Go & 1.21+ \\
Docker & 24.0+ \\
\bottomrule
\end{tabular}
\end{table}

\subheading{Execution instructions}

All models are containerized for reproducibility:

\begin{verbatim}
cd models/
make build           # Build Docker containers
make test            # Run quick validation
make run             # Run baseline ABM experiment
make sweep           # Parameter sensitivity analysis
make motivation-scale    # Scale degradation analysis
make montecarlo-alignment # Long-horizon Monte Carlo
\end{verbatim}

\subheading{Random seeds}

All stochastic simulations use fixed seeds for reproducibility. Default seed = 42 for single runs; parameter sweeps use seeds 0--$n$ for $n$ replications. Results in this paper are reproducible to machine precision given identical software versions.

